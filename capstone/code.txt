
// Relative Path: backend\app\api\agent_systems.py
from __future__ import annotations

import uuid
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from ..core.registry import AgentSystemRegistry
from ..core.builder import build_agent_system_from_yaml
from ..config import get_openai_api_key
from ..schemas.agent_system import AgentSystem


router = APIRouter()
registry = AgentSystemRegistry()


class RegisterResponse(BaseModel):
    id: str


@router.post("/agent-systems", response_model=RegisterResponse, status_code=201)
async def register_agent_system(doc: AgentSystem) -> RegisterResponse:
    try:
        _ = get_openai_api_key()
        resolved = build_agent_system_from_yaml(doc.model_dump())
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

    system_id = str(doc.system.get("name") or doc.system.get("id") or uuid.uuid4())

    def _factory():
        # Build fresh instance each time
        return build_agent_system_from_yaml(doc.model_dump())["instance"]

    registry.register(system_id, _factory, {"config": doc.model_dump(), "resolved": {"agents": resolved["agents"]}})
    return RegisterResponse(id=system_id)


@router.get("/agent-systems/{system_id}")
async def get_agent_system(system_id: str) -> dict:
    res = registry.get_resolved(system_id)
    if not res:
        raise HTTPException(status_code=404, detail="not found")
    return res






// Relative Path: backend\app\api\sessions.py
from __future__ import annotations

import asyncio
import json
import uuid
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from starlette.responses import StreamingResponse, FileResponse
from fastapi import HTTPException

from ..core.session_store import SessionStore
from ..core.registry import AgentSystemRegistry
from capstone.prototype.todolist_md import get_todolist_path


# create a logger use structlog
import structlog
logger = structlog.get_logger()

router = APIRouter()
sessions = SessionStore()
# Reuse the same registry instance used by agent_systems module
from .agent_systems import registry as systems


class CreateSessionRequest(BaseModel):
    agent_system_id: str
    user_id: str | None = None


class CreateSessionResponse(BaseModel):
    sid: str


@router.post("/sessions", response_model=CreateSessionResponse, status_code=201)
async def create_session(body: CreateSessionRequest) -> CreateSessionResponse:
    factory = systems.get_factory(body.agent_system_id)
    if not factory:
        raise HTTPException(status_code=404, detail="agent system not found")
    agent = factory()
    sid = str(uuid.uuid4())
    sessions.create(sid, agent)
    return CreateSessionResponse(sid=sid)


class MessageBody(BaseModel):
    text: str


@router.post("/sessions/{sid}/messages", status_code=202)
async def post_message(sid: str, body: MessageBody) -> dict:
    entry = sessions.get(sid)
    if not entry:
        raise HTTPException(status_code=404, detail="session not found")
    # Store last message to be picked up by /stream
    entry.agent.context["recent_user_message"] = body.text
    sessions.touch(sid)
    return {"status": "accepted"}


@router.get("/sessions/{sid}/stream")
async def stream(sid: str) -> StreamingResponse:
    entry = sessions.get(sid)
    if not entry:
        raise HTTPException(status_code=404, detail="session not found")

    async def event_gen():
        user_text = entry.agent.context.get("recent_user_message", "")
        logger.info("user_text", user_text=user_text)
        async for chunk in entry.agent.process_request(user_text, session_id=sid):
            # Ensure each event is a string line; chunk may already be text
            payload = chunk if isinstance(chunk, str) else str(chunk)
            yield f"data: {json.dumps(payload)}\n\n"

    return StreamingResponse(event_gen(), media_type="text/event-stream")


class AnswerBody(BaseModel):
    text: str


@router.post("/sessions/{sid}/answers", status_code=202)
async def post_answer(sid: str, body: AnswerBody) -> dict:
    entry = sessions.get(sid)
    if not entry:
        raise HTTPException(status_code=404, detail="session not found")
    entry.agent.context["recent_user_message"] = body.text
    sessions.touch(sid)
    return {"status": "accepted"}


@router.get("/sessions/{sid}/state")
async def get_state(sid: str) -> dict:
    entry = sessions.get(sid)
    if not entry:
        raise HTTPException(status_code=404, detail="session not found")
    ctx = entry.agent.context or {}
    return {
        "version": ctx.get("version"),
        "awaiting_user_input": ctx.get("awaiting_user_input"),
        "tasks": ctx.get("tasks", []),
        "blocker": ctx.get("blocker"),
    }


@router.get("/sessions/{sid}/artifacts/todolist.md")
async def get_todolist(sid: str) -> FileResponse:
    path = get_todolist_path(session_id=sid)
    try:
        # get_todolist_path returns a Path. If it doesn't exist, return 404 instead of raising at FileResponse.
        if not path.exists():
            raise HTTPException(status_code=404, detail="artifact not found")
        return FileResponse(str(path))
    except HTTPException:
        raise
    except Exception:
        # For any unexpected error, still return 404 to keep API surface stable
        raise HTTPException(status_code=404, detail="artifact not found")






// Relative Path: backend\app\api\stream.py
from __future__ import annotations

from fastapi import APIRouter


# Placeholder if we later split streaming concerns
router = APIRouter()






// Relative Path: backend\app\api\tools.py
from __future__ import annotations

from fastapi import APIRouter

from capstone.prototype.tools_builtin import BUILTIN_TOOLS
from capstone.prototype.tools import export_openai_tools


router = APIRouter()


@router.get("/tools")
async def list_tools() -> dict:
    tools = export_openai_tools(BUILTIN_TOOLS)
    # Normalize to a concise schema for UI consumption
    items = []
    for t in tools:
        fn = t.get("function", {})
        items.append(
            {
                "name": fn.get("name"),
                "description": fn.get("description"),
                "parameters": fn.get("parameters"),
            }
        )
    return {"tools": items}






// Relative Path: backend\app\api\__init__.py






// Relative Path: backend\app\core\builder.py
from __future__ import annotations

from typing import Any, Dict, List

from capstone.prototype.tools_builtin import ALL_TOOLS_WITH_AGENTS
from capstone.prototype.tools import ToolSpec, build_tool_index
from capstone.prototype.agent import ReActAgent
from capstone.prototype.llm_provider import OpenAIProvider  # type: ignore[attr-defined]
from ..config import get_openai_api_key


def _normalize(name: str) -> str:
    return name.strip().lower().replace("-", "_").replace(" ", "_")


def _resolve_tools(allowed: List[str] | None) -> List[ToolSpec]:
    names = set(_normalize(x) for x in (allowed or []))
    out: List[ToolSpec] = []
    for t in ALL_TOOLS_WITH_AGENTS:
        if not names or _normalize(t.name) in names or any(_normalize(a) in names for a in (t.aliases or [])):
            out.append(t)
    return out


def build_agent_system_from_yaml(doc: Dict[str, Any]) -> Dict[str, Any]:
    system = doc.get("system", {})
    agents_cfg = doc.get("agents", [])

    # Build orchestrator and sub-agents
    sub_agents: Dict[str, ReActAgent] = {}
    orchestrator_cfg = None
    for a in agents_cfg:
        if str(a.get("role", "")).lower() == "orchestrator":
            orchestrator_cfg = a
        else:
            sub_agents[a["id"]] = _build_agent(a)

    if orchestrator_cfg is None:
        raise ValueError("orchestrator agent not defined")

    orchestrator = _build_agent(orchestrator_cfg)

    # Attach sub-agents as tools where referenced by allow list
    allow = (orchestrator_cfg.get("tools") or {}).get("allow") or []
    for sub_id, sub in sub_agents.items():
        if sub_id in allow or _normalize(sub_id) in {_normalize(x) for x in allow}:
            orchestrator.tools.append(
                sub.to_tool(
                    name=sub_id,
                    description=sub_id,
                    allowed_tools=[t.name for t in sub.tools],
                    budget={"max_steps": sub.max_steps},
                    mission_override=sub.mission_text or None,
                )
            )

    # Rebuild tool index, prompt, and executor capabilities now that tools are attached
    try:
        orchestrator.tool_index = build_tool_index(orchestrator.tools)
        orchestrator.final_system_prompt = orchestrator._build_final_system_prompt()
        orchestrator.executor_index = orchestrator._build_executor_index()
        try:
            orchestrator.logger.info(
                "final_system_prompt",
                mode=str(orchestrator.prompt_overrides.get("mode", "compose")),
                prompt=orchestrator.final_system_prompt,
            )
        except Exception:
            pass
    except Exception:
        pass

    return {
        "system": system,
        "agents": {
            "orchestrator": orchestrator_cfg.get("id"),
            **{k: v for k, v in ((a.get("id"), a) for a in agents_cfg if a.get("id"))},
        },
        "instance": orchestrator,
    }


def _build_agent(cfg: Dict[str, Any]) -> ReActAgent:
    model = (cfg.get("model") or cfg.get("default_model") or {})
    provider = str(model.get("provider") or "openai").lower()
    if provider != "openai":
        raise ValueError("Only openai provider supported in v1")
    api_key = get_openai_api_key()
    llm = OpenAIProvider(api_key=api_key, model=str(model.get("model") or "gpt-4.1"), temperature=float(model.get("temperature", 0.1)))

    allow = (cfg.get("tools") or {}).get("allow") or []
    tools = _resolve_tools(allow)
    return ReActAgent(
        system_prompt=str(cfg.get("system_prompt") or ""),
        llm=llm,
        tools=tools,
        max_steps=int(cfg.get("max_steps", 50)),
        mission=str(cfg.get("mission") or ""),
    )






// Relative Path: backend\app\core\registry.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, Optional


@dataclass
class AgentSystemEntry:
    id: str
    factory: Callable[[], Any]
    resolved: Dict[str, Any]


class AgentSystemRegistry:
    def __init__(self) -> None:
        self._items: Dict[str, AgentSystemEntry] = {}

    def register(self, system_id: str, factory: Callable[[], Any], resolved: Dict[str, Any]) -> None:
        self._items[system_id] = AgentSystemEntry(id=system_id, factory=factory, resolved=resolved)

    def get_factory(self, system_id: str) -> Optional[Callable[[], Any]]:
        item = self._items.get(system_id)
        return item.factory if item else None

    def get_resolved(self, system_id: str) -> Optional[Dict[str, Any]]:
        item = self._items.get(system_id)
        return dict(item.resolved) if item else None

    def list_ids(self) -> list[str]:
        return list(self._items.keys())






// Relative Path: backend\app\core\session_store.py
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, Optional


@dataclass
class SessionEntry:
    """
    Represents a single session entry in the session store.

    Attributes:
        id (str): The unique identifier for the session.
        agent (Any): The agent instance associated with this session.
        created_at (str): The ISO-formatted timestamp when the session was created.
        last_activity (str): The ISO-formatted timestamp of the last activity in this session.
    """
    id: str
    agent: Any
    created_at: str
    last_activity: str


class SessionStore:
    """
    Manages the lifecycle and storage of session entries.

    This class provides methods to create, retrieve, and update session entries,
    which are stored in an in-memory dictionary keyed by session ID.
    """

    def __init__(self) -> None:
        """
        Initializes the session store with an empty dictionary to hold session entries.
        """
        self._items: Dict[str, SessionEntry] = {}

    def create(self, sid: str, agent: Any) -> SessionEntry:
        """
        Creates a new session entry and stores it in the session store.

        Args:
            sid (str): The unique session identifier.
            agent (Any): The agent instance to associate with this session.

        Returns:
            SessionEntry: The newly created session entry.
        """
        now = datetime.utcnow().isoformat()
        entry = SessionEntry(id=sid, agent=agent, created_at=now, last_activity=now)
        self._items[sid] = entry
        return entry

    def get(self, sid: str) -> Optional[SessionEntry]:
        """
        Retrieves a session entry by its session ID.

        Args:
            sid (str): The session identifier.

        Returns:
            Optional[SessionEntry]: The session entry if found, otherwise None.
        """
        return self._items.get(sid)

    def touch(self, sid: str) -> None:
        """
        Updates the last_activity timestamp of a session entry to the current time.

        Args:
            sid (str): The session identifier.

        This method does nothing if the session ID does not exist in the store.
        """
        if sid in self._items:
            self._items[sid].last_activity = datetime.utcnow().isoformat()






// Relative Path: backend\app\schemas\agent_system.py
from __future__ import annotations

from typing import Any, Dict, List, Optional
from pydantic import BaseModel


class ModelConfig(BaseModel):
    provider: str
    model: str
    temperature: float = 0.1


class AgentConfig(BaseModel):
    id: str
    role: str
    description: Optional[str] = None
    system_prompt: Optional[str] = None
    mission: Optional[str] = None
    max_steps: int = 50
    tools: Dict[str, List[str]] | None = None  # {allow: [..]}
    model: Optional[ModelConfig] = None


class AgentSystem(BaseModel):
    version: int
    system: Dict[str, Any] = {}
    agents: List[AgentConfig]






// Relative Path: backend\app\schemas\__init__.py






// Relative Path: backend\app\config.py
from __future__ import annotations

import os


def get_openai_api_key() -> str:
    key = os.getenv("OPENAI_API_KEY", "").strip()
    if not key:
        raise RuntimeError("OPENAI_API_KEY is not set")
    return key






// Relative Path: backend\app\main.py
from __future__ import annotations

from fastapi import FastAPI
from .api.tools import router as tools_router
from .api.agent_systems import router as agent_systems_router
from .api.sessions import router as sessions_router
from fastapi.middleware.cors import CORSMiddleware


def create_app() -> FastAPI:
    app = FastAPI(title="Agent Orchestration API", version="1.0")

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    @app.get("/health")
    async def health() -> dict:
        return {"status": "ok"}

    app.include_router(tools_router)
    app.include_router(agent_systems_router)
    app.include_router(sessions_router)

    return app


app = create_app()






// Relative Path: backend\app\__init__.py
from .main import app, create_app






// Relative Path: examples\idp_pack\idp_tools.py
from __future__ import annotations

from typing import List

from capstone.prototype.tools_builtin import BUILTIN_TOOLS_SIMPLIFIED
from capstone.prototype.tools import ToolSpec  # type: ignore
from capstone.prototype.tools_builtin import BUILTIN_TOOLS  # type: ignore


def get_idp_tools() -> List["ToolSpec"]:
	"""Return the built-in IDP tools for example composition."""
	return BUILTIN_TOOLS_SIMPLIFIED




// Relative Path: examples\idp_pack\rich_idp_cli.py
"""
Rich-based CLI for IDP Pack - Enhanced Git workflow interface
"""
from __future__ import annotations

import asyncio
import os
from pathlib import Path
from typing import Optional

from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from rich.text import Text
from rich.live import Live
from rich.spinner import Spinner
from rich.layout import Layout
from rich.table import Table
from rich.markdown import Markdown
from rich.syntax import Syntax

from capstone.examples.idp_pack.idp_tools import get_idp_tools
from capstone.prototype.agent import ReActAgent
from capstone.prototype.llm_provider import OpenAIProvider


class RichIDPCLI:
    """Rich-enhanced CLI for IDP Agent interactions."""
    
    def __init__(self):
        self.console = Console()
        self.agent: Optional[ReActAgent] = None
        self.session_id: Optional[str] = None
        
    def load_text(self, path: Path) -> str:
        """Load text file content."""
        return path.read_text(encoding="utf-8")
    
    async def initialize_agent(self) -> bool:
        """Initialize the ReAct agent with proper error handling."""
        try:
            root = Path(__file__).resolve().parents[2]
            prompt_path = root / "examples" / "idp_pack" / "system_prompt_git.txt"
            generic_path = root / "examples" / "idp_pack" / "system_prompt_idp.txt"
            orch_path = root / "examples" / "idp_pack" / "prompts" / "orchestrator.txt"
            
            if not prompt_path.exists():
                self.console.print(f"[red]Error: System prompt not found at {prompt_path}[/red]")
                return False
                
            git_mission = self.load_text(prompt_path)
            system_prompt = self.load_text(generic_path)
            orch_mission = self.load_text(orch_path)
            openai_key = os.getenv("OPENAI_API_KEY")
            
            if not openai_key:
                self.console.print("[yellow]Warning: No OPENAI_API_KEY found. Using mock provider.[/yellow]")
            
            provider = OpenAIProvider(api_key=openai_key)

            # Build sub-agent with Git tools
            git_tools = get_idp_tools()
            git_agent = ReActAgent(
                system_prompt=system_prompt,
                llm=provider,
                tools=git_tools,
                mission=git_mission,
            )

            # Orchestrator only exposes the sub-agent tool
            self.agent = ReActAgent(
                system_prompt=system_prompt,
                llm=provider,
                tools=[
                    git_agent.to_tool(
                        name="agent_git",
                        description="Git sub-agent",
                        allowed_tools=[t.name for t in git_tools],
                        budget={"max_steps": 12},
                        mission_override=git_mission,
                    )
                ],
                mission=orch_mission,
            )
            return True
            
        except Exception as e:
            self.console.print(f"[red]Failed to initialize agent: {e}[/red]")
            return False
    
    def show_welcome(self):
        """Display welcome screen with Rich formatting."""
        welcome_panel = Panel(
            Text.from_markup(
                "[bold blue]IDP Pack CLI[/bold blue]\n"
                "[dim]Intelligent Development Partner - Git Workflow Assistant[/dim]\n\n"
                "[green]Commands:[/green]\n"
                "• Type your request naturally\n"
                "• [bold]help[/bold] - Show available commands\n"
                "• [bold]status[/bold] - Show current session status\n"
                "• [bold]clear[/bold] - Clear session history\n"
                "• [bold]exit/quit/q[/bold] - Exit the CLI\n\n"
                "[yellow]Ready to assist with your Git workflow![/yellow]"
            ),
            title="🤖 Welcome",
            border_style="blue",
            padding=(1, 2)
        )
        self.console.print(welcome_panel)
    
    def show_status(self):
        """Show current session status."""
        table = Table(title="Session Status", show_header=True, header_style="bold magenta")
        table.add_column("Property", style="cyan", no_wrap=True)
        table.add_column("Value", style="white")
        
        table.add_row("Session ID", str(self.session_id) if self.session_id else "None")
        table.add_row("Agent Status", "Ready" if self.agent else "Not initialized")
        table.add_row("OpenAI Key", "✅ Set" if os.getenv("OPENAI_API_KEY") else "❌ Not set")
        
        self.console.print(table)
    
    def show_help(self):
        """Display help information."""
        help_content = """
        # IDP Pack CLI Help
        
        ## Available Commands
        - **help** - Show this help message
        - **status** - Display current session information
        - **clear** - Clear the current session and start fresh
        - **exit/quit/q** - Exit the CLI
        
        ## Usage Examples
        - "Show me the current git status"
        - "Create a new branch for feature X"
        - "Review the last commit"
        - "Help me resolve merge conflicts"
        
        ## Tips
        - Use natural language to describe what you need
        - The agent can handle complex Git workflows
        - Session history is maintained for context
        """
        self.console.print(Markdown(help_content))
    
    async def process_user_input(self, user_input: str) -> bool:
        """Process user input and handle agent responses."""
        # Handle built-in commands
        if user_input.lower() in {"exit", "quit", "q"}:
            return False
        elif user_input.lower() == "help":
            self.show_help()
            return True
        elif user_input.lower() == "status":
            self.show_status()
            return True
        elif user_input.lower() == "clear":
            self.session_id = None
            self.console.print("[green]Session cleared![/green]")
            return True
        
        # Process with agent
        if not self.agent:
            self.console.print("[red]Agent not initialized![/red]")
            return True
        
        # Show processing indicator
        with Live(Spinner("dots", text="Processing..."), console=self.console) as live:
            try:
                response_text = ""
                async for update in self.agent.process_request(user_input, session_id=self.session_id):
                    response_text += update
                
                # Update session ID
                self.session_id = self.agent.session_id
                
                # Display response in a panel
                if response_text.strip():
                    response_panel = Panel(
                        response_text.strip(),
                        title="🤖 Agent Response",
                        border_style="green",
                        padding=(1, 2)
                    )
                    live.stop()
                    self.console.print(response_panel)
                
                # Check if awaiting user input
                if self.agent.context.get("awaiting_user_input"):
                    self.console.print("[yellow]Agent is awaiting additional input...[/yellow]")
                
            except Exception as e:
                live.stop()
                self.console.print(f"[red]Error processing request: {e}[/red]")
        
        return True
    
    async def run(self):
        """Main CLI loop."""
        self.show_welcome()
        
        # Initialize agent
        if not await self.initialize_agent():
            self.console.print("[red]Failed to initialize. Exiting.[/red]")
            return
        
        self.console.print("[green]✅ Agent initialized successfully![/green]\n")
        
        # Main interaction loop
        while True:
            try:
                user_input = Prompt.ask(
                    "[bold cyan]You[/bold cyan]",
                    console=self.console
                ).strip()
                
                if not user_input:
                    continue
                
                should_continue = await self.process_user_input(user_input)
                if not should_continue:
                    break
                    
            except KeyboardInterrupt:
                self.console.print("\n[yellow]Interrupted by user[/yellow]")
                break
            except Exception as e:
                self.console.print(f"[red]Unexpected error: {e}[/red]")
                break
        
        # Goodbye message
        goodbye_panel = Panel(
            "[bold blue]Thank you for using IDP Pack CLI![/bold blue]\n"
            "[dim]Your intelligent development partner[/dim]",
            title="👋 Goodbye",
            border_style="blue"
        )
        self.console.print(goodbye_panel)


async def main():
    """Entry point for the Rich IDP CLI."""
    cli = RichIDPCLI()
    await cli.run()


if __name__ == "__main__":
    asyncio.run(main())



// Relative Path: examples\idp_pack\run_idp_cli.py
from __future__ import annotations

import asyncio
import os
from pathlib import Path
from typing import Optional


from capstone.examples.idp_pack.idp_tools import get_idp_tools
from capstone.prototype.agent import ReActAgent
from capstone.prototype.llm_provider import OpenAIProvider
from capstone.prototype.tools_builtin import AGENT_TOOLS




def load_text(path: Path) -> str:
	"""Load text file content."""
	return path.read_text(encoding="utf-8")


async def main() -> None:
	root = Path(__file__).resolve().parents[2]
	# Orchestrator mission
	orch_path = root / "examples" / "idp_pack" / "prompts" / "orchestrator.txt"
	orch_system_prompt = load_text(orch_path)

	mission_path = root / "examples" / "idp_pack" / "prompts" / "mission_git.txt"
	mission = load_text(mission_path)

    # Initialize LLM provider with fallback to mock if no API key
	openai_key = os.getenv("OPENAI_API_KEY")

	provider = OpenAIProvider(api_key=openai_key)

	# Sub-agent with Git tools (now with explicit Git mission)
	git_tools = get_idp_tools()
	git_agent = ReActAgent(
		system_prompt=None,
		llm=provider,
		tools=git_tools,
		mission=mission,
	)

	# Orchestrator only knows the sub-agent tool (delegation)
	orchestrator = ReActAgent(
		system_prompt=orch_system_prompt,
		llm=provider,
		tools=[
			git_agent.to_tool(
				name="agent_git",
				description="Git sub-agent",
				allowed_tools=[t.name for t in git_tools],
				budget={"max_steps": 12},
				mission_override=mission,
			),
		],
		mission=mission,
	)

	print("=" * 80)
	print("IDP Pack CLI - minimal Git workflow example")
	print("Type 'exit' to quit.")
	print("=" * 80)

	session_id: Optional[str] = None
	print("\nRoles:\n- Orchestrator (delegates)\n- Sub-Agent (does the work)\n")
	while True:
		msg = input("You: ").strip()
		if msg.lower() in {"", "exit", "quit", "q"}:
			break
		async for update in orchestrator.process_request(msg, session_id=session_id):
			print(update, end="", flush=True)
		session_id = orchestrator.session_id
		if orchestrator.context.get("awaiting_user_input"):
			continue
		print("")


if __name__ == "__main__":
	asyncio.run(main())




// Relative Path: frontend\components\sse_chat.py
import json
from typing import Generator

import requests
from sseclient import SSEClient


def stream_agent(base_url: str, session_id: str) -> Generator[str, None, None]:
    """Stream text chunks from the backend SSE endpoint.

    Parameters
    ----------
    base_url: str
        The FastAPI service base URL, e.g. http://localhost:8000
    session_id: str
        The session identifier to stream from

    Yields
    ------
    str
        Incremental text chunks suitable for direct UI rendering
    """
    url = f"{base_url.rstrip('/')}/sessions/{session_id}/stream"
    with requests.get(url, stream=True, timeout=300) as response:
        response.raise_for_status()
        client = SSEClient(response)
        for event in client.events():
            if not event.data:
                continue
            try:
                payload = json.loads(event.data)
            except Exception:
                payload = event.data
            yield str(payload)






// Relative Path: frontend\components\__init__.py






// Relative Path: frontend\streamlit_app.py
import os
import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests
import streamlit as st
import yaml

from components.sse_chat import stream_agent


def get_default_base_url() -> str:
    """Return default backend base URL, reading from AGENT_API_BASE_URL if present."""
    return os.getenv("AGENT_API_BASE_URL", "http://localhost:8000")


def init_session_state() -> None:
    """Initialize Streamlit session state keys used by the app."""
    defaults: Dict[str, Any] = dict(
        agent_system_id=None,
        session_id=None,
        chat_history=[],
        last_state=None,
        todolist_md=None,
    )
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


def post_agent_system(base_url: str, payload: Dict[str, Any]) -> Tuple[bool, Optional[str], str]:
    """POST /agent-systems and return (ok, id, error_text)."""
    try:
        url = f"{base_url.rstrip('/')}/agent-systems"
        resp = requests.post(url, json=payload, timeout=30)
        if resp.status_code == 201:
            return True, resp.json().get("id"), ""
        return False, None, f"{resp.status_code}: {resp.text}"
    except Exception as exc:  # noqa: BLE001
        return False, None, str(exc)


def get_agent_system(base_url: str, system_id: str) -> Tuple[bool, Optional[Dict[str, Any]], str]:
    """GET /agent-systems/{id}."""
    try:
        url = f"{base_url.rstrip('/')}/agent-systems/{system_id}"
        resp = requests.get(url, timeout=15)
        if resp.ok:
            return True, resp.json(), ""
        return False, None, f"{resp.status_code}: {resp.text}"
    except Exception as exc:  # noqa: BLE001
        return False, None, str(exc)


def create_session(base_url: str, agent_system_id: str) -> Tuple[bool, Optional[str], str]:
    """POST /sessions with agent_system_id and return (ok, sid, err)."""
    try:
        url = f"{base_url.rstrip('/')}/sessions"
        body = {"agent_system_id": agent_system_id}
        resp = requests.post(url, json=body, timeout=15)
        if resp.status_code == 201:
            return True, resp.json().get("sid"), ""
        return False, None, f"{resp.status_code}: {resp.text}"
    except Exception as exc:  # noqa: BLE001
        return False, None, str(exc)


def post_message(base_url: str, session_id: str, text: str) -> Tuple[bool, str]:
    """POST /sessions/{sid}/messages and return (ok, err)."""
    try:
        url = f"{base_url.rstrip('/')}/sessions/{session_id}/messages"
        resp = requests.post(url, json={"text": text}, timeout=15)
        if resp.status_code in (200, 202):
            return True, ""
        return False, f"{resp.status_code}: {resp.text}"
    except Exception as exc:  # noqa: BLE001
        return False, str(exc)


def get_state(base_url: str, session_id: str) -> Tuple[bool, Optional[Dict[str, Any]], str]:
    """GET /sessions/{sid}/state."""
    try:
        url = f"{base_url.rstrip('/')}/sessions/{session_id}/state"
        resp = requests.get(url, timeout=15)
        if resp.ok:
            return True, resp.json(), ""
        return False, None, f"{resp.status_code}: {resp.text}"
    except Exception as exc:  # noqa: BLE001
        return False, None, str(exc)


def get_todolist_md(base_url: str, session_id: str) -> Tuple[bool, Optional[str], str, int]:
    """GET /sessions/{sid}/artifacts/todolist.md returning (ok, text, err, status_code)."""
    try:
        url = f"{base_url.rstrip('/')}/sessions/{session_id}/artifacts/todolist.md"
        resp = requests.get(url, timeout=15)
        if resp.status_code == 200:
            return True, resp.text, "", 200
        return False, None, resp.text, resp.status_code
    except Exception as exc:  # noqa: BLE001
        return False, None, str(exc), 0


def get_tools(base_url: str) -> Tuple[bool, Optional[Dict[str, Any]], str]:
    """GET /tools."""
    try:
        url = f"{base_url.rstrip('/')}/tools"
        resp = requests.get(url, timeout=15)
        if resp.ok:
            return True, resp.json(), ""
        return False, None, f"{resp.status_code}: {resp.text}"
    except Exception as exc:  # noqa: BLE001
        return False, None, str(exc)


def render_tab_agent_system(base_url: str) -> None:
    """Render the Agent System registration tab."""
    st.subheader("Agent System registrieren")
    st.caption("YAML ODER JSON einfügen. Wird direkt an POST /agent-systems gesendet.")

    def _read_prompt(rel_path: str, fallback: str) -> str:
        """Read prompt file relative to capstone root; return fallback on error."""
        try:
            root = Path(__file__).resolve().parents[1]
            text = (root / rel_path).read_text(encoding="utf-8")
            return text.strip()
        except Exception:
            return fallback.strip()

    def _indent_block(text: str, spaces: int = 6) -> str:
        """Indent every line of a multi-line string by N spaces for YAML literal blocks."""
        pad = " " * spaces
        return "\n".join(pad + line if line else pad for line in text.splitlines())

    orch_text = _read_prompt(
        "examples/idp_pack/prompts/orchestrator.txt",
        "You are a generic orchestration agent.",
    )
    mission_text = _read_prompt(
        "examples/idp_pack/prompts/mission_git.txt",
        "Create a repository locally and on GitHub; push initial commit.",
    )

    # Default YAML mirrors run_idp_cli.py structure and prompts
    default_yaml = f"""
version: 1
system:
  name: idp-orchestrator
agents:
  - id: orchestrator
    role: orchestrator
    description: Main orchestrator
    system_prompt: |
{_indent_block(orch_text)}

    mission: |
{_indent_block(mission_text)}

    max_steps: 40
    model:
      provider: openai
      model: gpt-4.1
      temperature: 0.1
    tools:
      allow:
        - agent_git
  - id: agent_git
    role: worker
    description: Git worker
    mission: |
{_indent_block(mission_text)}

    max_steps: 12
    model:
      provider: openai
      model: gpt-4.1
      temperature: 0.1
    tools:
      allow:
        - validate_project_name_and_type
        - create_repository
"""

    text = st.text_area("AgentSystem (YAML oder JSON)", value=default_yaml, height=320)

    col_a, col_b = st.columns([1, 1])
    with col_a:
        parse_as = st.radio("Format", ["YAML", "JSON"], horizontal=True)
    with col_b:
        register_btn = st.button("📥 Registrieren", type="primary")

    if register_btn:
        payload: Optional[Dict[str, Any]]
        try:
            if parse_as == "YAML":
                payload = yaml.safe_load(text)
            else:
                payload = json.loads(text)
        except Exception as exc:  # noqa: BLE001
            st.error(f"Parsing-Fehler: {exc}")
            payload = None

        if payload is not None:
            ok, system_id, err = post_agent_system(base_url, payload)
            if ok and system_id:
                st.session_state.agent_system_id = system_id
                st.success(f"Registriert: {system_id}")
            else:
                st.error(f"Fehler {err}")

    if st.session_state.agent_system_id:
        with st.expander("📄 Resolved anzeigen (GET /agent-systems/{id})", expanded=False):
            ok, data, err = get_agent_system(base_url, st.session_state.agent_system_id)
            if ok and data is not None:
                st.json(data)
            else:
                st.warning(f"GET failed: {err}")


def render_tab_chat(base_url: str) -> None:
    """Render the Chat tab including session creation and SSE streaming."""
    st.subheader("Chat mit Orchestrator")

    if not st.session_state.agent_system_id:
        st.info("Bitte zuerst ein Agent System registrieren (Tab „Agent System“).")
        return

    col1, col2 = st.columns([1, 3])
    with col1:
        if st.button("🆕 Session erstellen"):
            ok, sid, err = create_session(base_url, st.session_state.agent_system_id)
            if ok and sid:
                st.session_state.session_id = sid
                st.session_state.chat_history = []
                st.success(f"Session: {sid}")
            else:
                st.error(f"Fehler: {err}")

    with col2:
        st.write(f"**Aktuelle Session:** `{st.session_state.session_id or '—'}`")

    user_msg = st.text_input(
        "Deine Nachricht",
        value=(
            "Erzeuge ein neues Service-Repo 'awesome-svc' und pushe initialen Commit."
        ),
        placeholder="Nachricht für den Orchestrator …",
    )
    go = st.button("Senden & Streamen ▶", type="primary")

    st.divider()
    chat_box = st.container(border=True)
    for role, content in st.session_state.chat_history:
        chat_box.markdown(f"**{role}:** {content}")

    if go:
        if not st.session_state.session_id:
            st.warning("Bitte zuerst eine Session erstellen.")
            return

        ok, err = post_message(base_url, st.session_state.session_id, user_msg)
        if not ok:
            st.error(f"Post fehlgeschlagen: {err}")
            return

        st.session_state.chat_history.append(("Du", user_msg))

        with st.status("Agent arbeitet …", expanded=True) as status_box:
            stream_area = st.empty()
            acc: List[str] = []
            try:
                for chunk in stream_agent(base_url, st.session_state.session_id):
                    acc.append(chunk)
                    stream_area.markdown("".join(acc))
            except Exception as exc:  # noqa: BLE001
                st.error(f"SSE-Fehler: {exc}")
            else:
                full = "".join(acc)
                st.session_state.chat_history.append(("Agent", full))
                status_box.update(label="Fertig.", state="complete")


def render_tab_state_and_todo(base_url: str) -> None:
    """Render the session state and ToDo artifact tab."""
    st.subheader("Session State & ToDo List")
    if not st.session_state.session_id:
        st.info("Zuerst eine Session erstellen.")
        return

    cols = st.columns(3)
    with cols[0]:
        if st.button("🔄 State laden"):
            ok, data, err = get_state(base_url, st.session_state.session_id)
            if ok and data is not None:
                st.session_state.last_state = data
            else:
                st.error(f"Fehler: {err}")

    with cols[1]:
        if st.button("📥 ToDo Markdown laden"):
            ok, text, err, status = get_todolist_md(base_url, st.session_state.session_id)
            if ok and text is not None:
                st.session_state.todolist_md = text
                st.markdown(text)
            else:
                if status == 404:
                    st.warning("Noch kein ToDo-Artifact vorhanden.")
                else:
                    st.error(f"Fehler: {err}")

    with cols[2]:
        if st.button("🔎 Raw ToDo als Link anzeigen"):
            url = f"{base_url.rstrip('/')}/sessions/{st.session_state.session_id}/artifacts/todolist.md"
            st.write(url)

    st.divider()
    st.write("Letzter State:")
    st.json(st.session_state.last_state or {})

    if st.session_state.todolist_md:
        st.divider()
        st.write("ToDo Markdown:")
        st.markdown(st.session_state.todolist_md)


def render_tab_tools(base_url: str) -> None:
    """Render the tools listing tab."""
    st.subheader("Verfügbare Tools")
    if st.button("🔄 Tools laden"):
        ok, data, err = get_tools(base_url)
        if ok and data is not None:
            st.json(data)
        else:
            st.error(f"Fehler: {err}")


def main() -> None:
    """Main entrypoint rendering the Streamlit application."""
    st.set_page_config(page_title="Agent Demo", page_icon="🤖", layout="wide")

    st.sidebar.header("Backend")
    base_url = st.sidebar.text_input(
        "Base URL",
        value=get_default_base_url(),
        help="FastAPI Base URL, z. B. http://localhost:8000",
    )
    st.sidebar.caption(
        "Tipp: OPENAI_API_KEY im Backend setzen; ggf. GITHUB_TOKEN für Repo-Erstellung."
    )

    init_session_state()

    st.title("🤖 Agent Orchestration Demo")
    tab_sys, tab_chat, tab_state, tab_tools = st.tabs(
        ["⚙️ Agent System", "💬 Chat", "📋 State & ToDo", "🧰 Tools"]
    )

    with tab_sys:
        render_tab_agent_system(base_url)
    with tab_chat:
        render_tab_chat(base_url)
    with tab_state:
        render_tab_state_and_todo(base_url)
    with tab_tools:
        render_tab_tools(base_url)


if __name__ == "__main__":
    main()






// Relative Path: prototype\agent.py
# ==================== PRODUCTION REACT AGENT ====================

import asyncio
from datetime import datetime
from enum import Enum
import hashlib
import json
import time
import re  # CHANGED: for simple fact extraction (kebab-case)
from typing import Any, AsyncGenerator, Dict, List, Optional, ClassVar

from prometheus_client import Counter, Gauge, Histogram
from pydantic import BaseModel, Field
from pydantic import field_validator
import structlog

from capstone.prototype.feedback_collector import FeedbackCollector
from capstone.prototype.llm_provider import LLMProvider
from capstone.prototype.statemanager import StateManager
from capstone.prototype.todolist_md import (
    render_todolist_markdown,
)
from capstone.prototype.tools import (
    ToolSpec,
    export_openai_tools,
    build_tool_index,
    execute_tool_by_name_from_index,
)

# removed: from turtle import tracer  # CHANGED: conflicting with opentelemetry tracer
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider

tracer = trace.get_tracer(__name__)

# Prometheus Metrics
workflow_counter = Counter('idp_workflows_started', 'Number of IDP workflows started')
workflow_success = Counter('idp_workflows_completed', 'Number of successful workflows')
workflow_failed = Counter('idp_workflows_failed', 'Number of failed workflows')
step_duration = Histogram('idp_step_duration_seconds', 'Duration of workflow steps', ['step_type'])
active_workflows = Gauge('idp_active_workflows', 'Number of currently active workflows')
tool_execution_time = Histogram('idp_tool_execution_seconds', 'Tool execution time', ['tool_name'])
tool_success_rate = Counter('idp_tool_success', 'Tool execution success', ['tool_name'])
tool_failure_rate = Counter('idp_tool_failure', 'Tool execution failures', ['tool_name'])

# ===== Default Generic Prompt (used in composed mode) =====
DEFAULT_GENERIC_PROMPT = (
    """
<GenericAgentSection>
You are a ReAct-style execution agent.

Operating principles:
- Plan-first: create/update a concise Todo List; clarify blocking questions first.
- Be deterministic, keep outputs minimal & actionable.
- After each tool call, update state; avoid loops; ask for help on blockers.

Decision policy:
- Prefer available tools; ask user only for truly blocking info.
- Stop when acceptance criteria for the mission are met.

Output style:
- Short, structured, CLI-friendly status lines.
</GenericAgentSection>
"""
).strip()

# ============ Action Space ============
class ActionType(Enum):
    TOOL_CALL = "tool_call"
    ASK_USER = "ask_user"
    COMPLETE = "complete"
    UPDATE_TODOLIST = "update_todolist"
    ERROR_RECOVERY = "error_recovery"

class ActionDecision(BaseModel):
    action_type: ActionType
    action_name: str
    parameters: Dict[str, Any] = Field(default_factory=dict)
    reasoning: str = ""
    confidence: float = Field(0.0, ge=0, le=1)

    @field_validator("action_type", mode="before")
    def _map_old_names(cls, v):
        if isinstance(v, str) and v.lower() == "update_checklist":
            return ActionType.UPDATE_TODOLIST
        return ActionType(v) if isinstance(v, str) else v

# Für die „Fragen zuerst?“-Heuristik lassen wir das LLM blocking Fragen strukturieren.
class BlockingQuestions(BaseModel):
    blocking: List[str] = Field(default_factory=list)
    optional: List[str] = Field(default_factory=list)


# ===== Structured Plan Models (Single Source of Truth) =====
class TaskStatus(str, Enum):
    PENDING = "PENDING"
    IN_PROGRESS = "IN_PROGRESS"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    SKIPPED = "SKIPPED"


class PlanTask(BaseModel):
    id: str
    title: str
    description: str | None = None
    tool: str | None = None
    executor_id: str | None = None
    action: str | None = None
    params: Dict[str, Any] | None = None
    status: TaskStatus = TaskStatus.PENDING
    depends_on: List[str] = Field(default_factory=list)
    priority: int | None = None
    notes: str | None = None
    owner_agent: str | None = None


class PlanOutput(BaseModel):
    tasks: List[PlanTask] = Field(default_factory=list)
    open_questions: List[str] = Field(default_factory=list)

# ============ Agent ============
class ReActAgent:
    def __init__(
        self,
        system_prompt: str | None,
        llm: LLMProvider,
        *,
        tools: List[ToolSpec] | None = None,
        max_steps: int = 50,
        mission: str | None = None,
        prompt_overrides: Dict[str, Any] | None = None,
    ):
        """
        Initializes the ReActAgent with the given system prompt, LLM provider, tools, and maximum steps.
        Args:
            system_prompt: The system prompt for the LLM.
            llm: The LLM provider.
            tools: The tools to use.
            max_steps: The maximum number of steps to take.
        """
        self.system_prompt_base = (system_prompt or "").strip()
        self.llm = llm
        self.tools: List[ToolSpec] = tools or []  # keine Default-Tools -> generisch
        self.tool_index = build_tool_index(self.tools)
        self.max_steps = max_steps

        # New prompt composition fields
        self.mission_text: str | None = (mission or None)
        self.prompt_overrides: Dict[str, Any] = dict(prompt_overrides or {})

        self.state = StateManager()
        self.feedback = FeedbackCollector()
        self.logger = structlog.get_logger()

        self.session_id: Optional[str] = None
        self.context: Dict[str, Any] = {}
        self.react_history: List[str] = []
        self.step = 0

        # Loop-Guard: keep recent action/observation signatures to detect repetition
        self._loop_signatures: List[str] = []
        # Duplicate-Action Guard: track executor/action/params hash and version
        self._duplicate_action_cache: Dict[str, int] = {}

        # Executor capabilities index (built from tools)
        self.executor_index: Dict[str, Dict[str, Any]] = {}

        # Build the final system prompt using the new builder (default: compose)
        self.final_system_prompt = self._build_final_system_prompt()
        try:
            self.logger.info(
                "final_system_prompt",
                mode=str(self.prompt_overrides.get("mode", "compose")),
                prompt=self.final_system_prompt,
            )
        except Exception:
            # Logging must never break agent construction
            pass

        # Build executor capabilities index at construction
        try:
            self.executor_index = self._build_executor_index()
        except Exception:
            self.executor_index = {}

    async def process_request(self, user_input: str, session_id: Optional[str] = None) -> AsyncGenerator[str, None]:
        """
        Verarbeitet eine Benutzeranfrage und liefert schrittweise Status-Updates als asynchronen Generator.

        Diese Methode übernimmt die Steuerung des gesamten Agenten-Workflows für eine Session:
        - Initialisiert oder lädt den Session-Kontext (inkl. Wiederherstellung eines bestehenden Zustands)
        - Erkennt, ob auf eine Benutzereingabe gewartet wird, und verarbeitet ggf. die Antwort
        - Hält den aktuellen Stand der Benutzereingabe und des augmentierten Kontexts aktuell
        - Startet die eigentliche Verarbeitungsschleife (_react_loop), die die Aufgabenplanung, Tool-Aufrufe und Interaktionen mit dem LLM übernimmt
        - Gibt nach jedem Verarbeitungsschritt ein Update-String zurück, sodass der Aufrufer den Fortschritt verfolgen kann
        - Markiert den Workflow als erfolgreich oder fehlgeschlagen und gibt entsprechende Statusmeldungen aus
        - Sichert am Ende den aktuellen Zustand und spült gesammeltes Feedback asynchron auf die Festplatte

        Args:
            user_input (str): Die aktuelle Benutzereingabe, die verarbeitet werden soll.
            session_id (Optional[str]): Eine optionale Sitzungs-ID, um den Zustand zwischen Aufrufen zu persistieren.

        Returns:
            AsyncGenerator[str, None]: Ein asynchroner Generator, der Status- und Fortschrittsmeldungen als Strings liefert.
        """
        try:
            self.session_id = session_id or hashlib.md5(f"{user_input}{time.time()}".encode()).hexdigest()
            restored = await self.state.load_state(self.session_id)
            if restored:
                self._restore(restored)
                self.context["recent_user_message"] = user_input
                if self.context.get("awaiting_user_input"):
                    # Persist reply & extract simple facts
                    self._store_user_reply(user_input)  # CHANGED
                    # keep an augmented request blob for LLM prompts
                    base_req = self.context.get("user_request", "")
                    known = self.context.get("known_answers_text", "")
                    self.context["user_request_augmented"] = f"{base_req}\n\nKnown answers from user:\n{known}".strip()  # CHANGED
                    yield "📥 Danke! Ich mache weiter…\n"
            else:
                # Preserve any pre-seeded flags (e.g., suppress_markdown/ephemeral_state) from callers
                ctx_init = dict(self.context or {})
                ctx_init.update({
                    "user_request": user_input,
                    "session_id": self.session_id,
                    "started_at": datetime.now().isoformat(),
                })
                self.context = ctx_init
                self.context["recent_user_message"] = user_input
                # initial augmented = original
                self.context["user_request_augmented"] = self.context["user_request"]  # CHANGED
                # initialize plan version for optimistic concurrency (sub-agent patches)
                self.context["version"] = 1
                yield f"🚀 Neue Session: {self.session_id}\n"

            yield f"📝 Verarbeitung: {user_input}\n"
            async for update in self._react_loop():
                yield update

            # CHANGED: Only mark success if we're not waiting for user input
            if self.context.get("awaiting_user_input"):
                yield "\n⏸️ Warte auf deine Antwort …\n"
            else:
                workflow_success.inc()
                yield "\n✅ Fertig!\n"

        except Exception as e:
            workflow_failed.inc()
            self.logger.error("workflow_failed", error=str(e))
            yield f"\n❌ Fehler: {e}\n"
        finally:
            active_workflows.dec()
            await self._save()
            await self.feedback.flush_feedback()

    # ===== main ReAct loop =====
    async def _react_loop(self) -> AsyncGenerator[str, None]:
        """
        Die Hauptschleife des ReAct-Agenten steuert die gesamte Ausführung des Workflows. 
        Sie übernimmt folgende Aufgaben:
        
        1. Prüft zu Beginn, ob für die Erstellung eines initialen Plans (Todo-Liste) noch zwingend benötigte Informationen vom Nutzer fehlen. 
           Falls ja, werden diese Fragen gesammelt und dem Nutzer präsentiert, bevor der Plan erstellt wird.
        2. Erstellt, sofern alle Pflichtangaben vorliegen, eine initiale Todo-Liste und speichert diese im Kontext.
        3. Durchläuft anschließend einen iterativen ReAct-Zyklus, in dem für jeden Schritt:
            - Ein neuer Gedanke ("Thought") vom LLM generiert wird, der die aktuelle Situation bewertet.
            - Basierend darauf eine Aktionsentscheidung getroffen wird (z.B. Tool-Aufruf, Nutzerfrage, Abschluss).
            - Die gewählte Aktion ausgeführt und das Ergebnis ("Observation") gesammelt wird.
            - Der Kontext mit den neuen Informationen aktualisiert wird.
        4. Die Schleife endet, sobald entweder der Workflow abgeschlossen ist (COMPLETE) oder eine Nutzerinteraktion erforderlich wird (ASK_USER).
        5. Nach jeweils fünf Schritten wird der aktuelle Zustand persistiert.
        
        Die Methode liefert fortlaufend Status- und Fortschrittsmeldungen als asynchronen Generator zurück, die z.B. für eine UI oder ein Monitoring genutzt werden können.
        """

        # PLAN-FIRST: Ermittele blocking Fragen; wenn vorhanden -> erst ASK_USER
        if not self.context.get("todolist_created"):
            questions = await self._detect_blocking_questions()
            if questions.blocking:
                msg = await self._handle_user_interaction(
                    "ask_user",
                    {"questions": questions.blocking, "context": "Benötigte Angaben, um den Plan zu erstellen."},
                )
                yield f"❓ {msg}\n"
                return  # warte auf User

            # Todo-Liste erzeugen (Plan) – optional offene Fragen separat listen
            await self._create_initial_plan(open_questions=questions.optional)
            yield f"🗂️ Todo-Liste erstellt: {self.context.get('todolist_file')}\n"

        # Regelmäßiger ReAct-Zyklus
        while self.step < self.max_steps:
            self.step += 1
            yield f"\n--- Schritt {self.step} ---\n"

            # Phase 5: If we have a pending sub-agent question and now user inputs, attempt resume
            if self.context.get("pending_subagent_query") and self.context.get("user_inputs"):
                try:
                    pq = dict(self.context.get("pending_subagent_query") or {})
                    last_answer = str(self.context.get("user_inputs", [])[-1].get("answer") or "").strip()
                    if last_answer:
                        # Build a resume call to the same sub-agent tool
                        tool_name = str(pq.get("tool") or pq.get("agent_name") or "").strip()
                        state_token = pq.get("state_token")
                        params = {
                            "task": pq.get("task") or "Continue previous sub-agent task",
                            "inputs": {},
                            "shared_context": {
                                "session_id": self.session_id,
                                "version": int(self.context.get("version", 1)),
                                "facts": self.context.get("facts", {}),
                                "known_answers_text": self.context.get("known_answers_text", ""),
                                "user_inputs": self.context.get("user_inputs", []),
                                "tasks": self._get_tasks(),
                            },
                            "resume_token": state_token,
                            "answers": {"latest": last_answer},
                        }
                        obs = await self._handle_tool(tool_name, params)
                        yield f"🔁 Resume Sub-Agent: {obs}\n"
                        self.context.pop("pending_subagent_query", None)
                except Exception:
                    pass

            thought = await self._generate_thought()
            yield f"💭 Thought:\n{thought}\n"

            decision = await self._decide_next_action()
            yield f"⚡ Aktion: {decision.action_type.value} — {decision.action_name}\n"
            yield f"   Grund: {decision.reasoning}\n"

            started = time.time()
            observation = await self._exec_with_retry(decision)
            step_duration.labels(step_type=decision.action_type.value).observe(time.time() - started)

            yield f"👀 Observation:\n{observation}\n"

            # Loop-Guard: Detect three identical action/observation pairs in a row
            try:
                if self._record_and_check_loop(decision, observation, window_size=3):
                    if int(self.context.get("loop_guard_cooldown", 0)) <= 2:
                        msg = await self._handle_user_interaction(
                            "ask_user",
                            {
                                "questions": [
                                    "Ich erkenne wiederholte, wirkungslose Schritte. Hast du zusätzliche Informationen oder soll ich die Strategie ändern?"
                                ],
                                "context": "Loop-Guard: Drei identische Aktionen/Observations in Folge erkannt."
                            },
                        )
                        yield f"❓ {msg}\n"
                    else:
                        self.context["loop_guard_cooldown"] = 0
                        yield "⚠️ Loop erkannt → Strategie gewechselt (kein weiteres ask_user)."
                    break
            except Exception as e:
                self.logger.warning("loop_guard_failed", error=str(e))

            await self._update_context(decision, observation)
            if decision.action_type in {ActionType.COMPLETE, ActionType.ASK_USER}:
                break

            if self.step % 5 == 0:
                await self._save()

    # ===== Plan First =====
    async def _detect_blocking_questions(self) -> BlockingQuestions:
        """
        Lässt das LLM prüfen, ob wesentliche Pflichtinfos für ERSTE(n) Tool-Schritt(e) fehlen.
        Rückgabe trennt blocking (vor Plan) und optional (als Open Questions in den Plan).
        """
        # CHANGED: include augmented request + known answers + facts
        req = self.context.get("user_request_augmented") or self.context.get("user_request", "")
        last_msg = self.context.get("recent_user_message", "")
        facts = self.context.get("facts", {})
        user_inputs_struct = list(self.context.get("user_inputs", []))
        known_structured = "\n".join(
            f"- {str(ui.get('answer') or '').strip()}" for ui in user_inputs_struct if str(ui.get('answer') or '').strip()
        )
        known_legacy = self.context.get("known_answers_text", "")

        required_by_tool = []
        for spec in self.tools:
            reqs = list((spec.input_schema or {}).get("required", []))
            if reqs:
                required_by_tool.append({"tool": spec.name, "required": reqs})

        prompt = (
            "You are a planning assistant. Given the user's request, the available tools with their required "
            "parameters, and the known answers provided by the user, identify:\n"
            "1) blocking questions (without answers you cannot even start the first 1–2 steps)\n"
            "2) optional questions (nice-to-have refinements)\n\n"
            "IMPORTANT:\n"
            "- Do NOT include questions that are already answered by 'Known answers' or 'Facts'.\n"
            "- Prefer to proceed without questions when enough information is available to start.\n\n"
            f"User request:\n{req}\n\n"
            f"Recent user message:\n{last_msg}\n\n"
            f"Known answers (from structured inputs):\n{known_structured or known_legacy or '- none -'}\n\n"
            f"Facts (parsed):\n{json.dumps(facts, ensure_ascii=False, indent=2)}\n\n"
            f"User inputs (structured):\n{json.dumps(user_inputs_struct, ensure_ascii=False, indent=2)}\n\n"
            f"Tools and required params:\n{json.dumps(required_by_tool, ensure_ascii=False, indent=2)}"
        )
        try:
            return await self.llm.generate_structured_response(prompt, BlockingQuestions, system_prompt=self.final_system_prompt)
        except Exception:
            return BlockingQuestions()

    async def _create_initial_plan(self, *, open_questions: List[str]):
        """
        Erstellt den initialen Plan als strukturierte JSON-Struktur und rendert die Markdown-Ansicht deterministisch.

        Args:
            open_questions: Nicht-blockierende Fragen, die zusätzlich gelistet werden sollen.
        """
        user_req = self.context.get("user_request_augmented") or self.context.get("user_request", "")
        # Expose executor capabilities to the planner (with descriptions)
        exec_index = self.executor_index or {}
        visible_executors = [(eid, (exec_index[eid] or {}).get("label") or eid) for eid in exec_index.keys()]
        capabilities_map = {eid: [a.get("name") for a in (exec_index[eid].get("actions") or [])] for eid in exec_index.keys()}
        # Detailed map for the LLM with action descriptions and required params
        capabilities_detailed = {}
        for eid, cap in exec_index.items():
            actions_detailed = []
            for a in cap.get("actions", []) or []:
                params_schema = a.get("params_schema") or {}
                actions_detailed.append({
                    "name": a.get("name"),
                    "description": a.get("description") or "",
                    "required_params": list((params_schema.get("required") or [])),
                })
            capabilities_detailed[eid] = {
                "label": cap.get("label") or eid,
                "actions": actions_detailed,
            }
        prompt = (
            "Plan tasks for the user's request. Return JSON only, matching the schema.\n"
            "Guidelines:\n"
            "- Prefer 3–10 atomic, verifiable tasks.\n"
            "- Use only these executors and their actions.\n"
            "- Use action descriptions to decide scope: if one action can complete multiple subtasks, avoid splitting.\n"
            "- Avoid proposing tasks for unavailable executors/actions.\n"
            "- Prefer composite tools over manual multi-step sequences when available.\n"
            "- Avoid duplicate tasks (same executor_id + action + params).\n"
            "- Set executor_id from 'visible_executors' and action from 'capabilities_map[executor_id]'.\n"
            "- Include the exact tool name when applicable in field 'tool' (optional).\n"
            "- Provide stable 'id' (e.g., t1, t2), 'title', optional 'params', and initial 'status' = PENDING.\n"
            "- Fill 'open_questions' with clarifications that are nice-to-have (non-blocking).\n\n"
            f"User request:\n{user_req}\n\n"
            f"visible_executors = {json.dumps(visible_executors, ensure_ascii=False)}\n"
            f"capabilities_map = {json.dumps(capabilities_map, ensure_ascii=False)}\n"
            f"capabilities_detailed = {json.dumps(capabilities_detailed, ensure_ascii=False)}\n"
        )
        try:
            plan: PlanOutput = await self.llm.generate_structured_response(prompt, PlanOutput, system_prompt=self.final_system_prompt)
        except Exception:
            # Fallback to empty plan; we can proceed with tool decisions later
            plan = PlanOutput(tasks=[], open_questions=[])

        # Merge optional open questions detected earlier
        merged_oq = list((plan.open_questions or [])) + list(open_questions or [])
        # Ensure unique order-preserving
        seen = set()
        final_oq: List[str] = []
        for q in merged_oq:
            if q not in seen:
                final_oq.append(q)
                seen.add(q)

        # Reconcile, validate, and prune against capabilities
        tasks_dicts = [t.model_dump() for t in plan.tasks]
        try:
            tasks_dicts = self._reconcile_tasks_with_capabilities(tasks_dicts, self.executor_index)
            _ = self._validate_plan_against_capabilities(tasks_dicts, self.executor_index)
            tasks_dicts = self._prune_invalid_and_duplicate_tasks(tasks_dicts, self.executor_index)
        except Exception:
            pass

        # Persist authoritative state in context (as plain dicts for easy serialization)
        self.context["tasks"] = tasks_dicts
        self.context["open_questions"] = final_oq
        self.context["todolist_created"] = True

        # Render Markdown view unless suppressed (e.g., in sub-agent sandbox)
        if not self.context.get("suppress_markdown"):
            path = render_todolist_markdown(
                tasks=self.context["tasks"],
                open_questions=self.context.get("open_questions", []),
                session_id=self.session_id,
            )
            self.context["todolist_file"] = path

    # ===== ReAct inner pieces =====
    async def _generate_thought(self) -> str:
        summary = self._summary_for_llm()
        prompt = (
            f"Context:\n{summary}\n\n"
            "Think step by step about the single next best move. Consider the Todo List, tool availability, and errors. "
            "Keep it short."
        )
        try:
            return await self.llm.generate_response(prompt, system_prompt=self.final_system_prompt)
        except Exception:
            return "Analyze state and choose next safe, useful step."

    async def _decide_next_action(self) -> ActionDecision:
        summary = self._summary_for_llm()
        todostate = "created" if self.context.get("todolist_created") else "missing"

        # 1) Anbieter-Tool-Calling
        try:
            tools_fc = export_openai_tools(self.tools)
            meta = [
                {  # ✅ gültiges Schema: leeres Objekt erlaubt beliebige Felder
                "type": "function",
                "function": {
                    "name": "update_todolist",
                    "description": "Create or modify the Todo List",
                    "parameters": {
                        "type": "object",
                        "properties": {},               # <- wichtig
                        "additionalProperties": True
                    }
                }
                },
                {  # ✅ mit Properties
                "type": "function",
                "function": {
                    "name": "ask_user",
                    "description": "Ask user for information",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "questions": {"type": "array", "items": {"type": "string"}},
                            "context": {"type": "string"}
                        },
                        "required": ["questions"],
                        "additionalProperties": False
                    }
                }
                },
                {  # ✅ optionales summary
                "type": "function",
                "function": {
                    "name": "complete",
                    "description": "Finish the workflow",
                    "parameters": {
                        "type": "object",
                        "properties": { "summary": {"type": "string"} },
                        "additionalProperties": False
                    }
                }
                },
                {  # ✅ generisches Objekt
                "type": "function",
                "function": {
                    "name": "error_recovery",
                    "description": "Attempt generic error recovery",
                    "parameters": {
                        "type": "object",
                        "properties": {},
                        "additionalProperties": True
                    }
                }
                },
                {  # ✅ delegate_to_agent meta
                "type": "function",
                "function": {
                    "name": "delegate_to_agent",
                    "description": "Delegate a sub-task to a specialized sub-agent (registered as a tool)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "agent_name": {"type": "string"},
                            "task": {"type": "string"},
                            "inputs": {"type": "object"},
                            "allowed_tools": {"type": "array", "items": {"type": "string"}},
                            "budget": {"type": "object"}
                        },
                        "required": ["agent_name", "task"],
                        "additionalProperties": True
                    }
                }
                },
            ]
            call = await self.llm.call_tools(
                system_prompt=self.final_system_prompt,
                tools=tools_fc + meta,
                messages=[{
                    "role": "user",
                    "content": f"Context:\n{summary}\n\nTodoList: {todostate}\n"
                               "Call exactly one function for the next action."
                }],
            )
            if call:
                name = (call.get("name") or "").lower().strip()
                params = call.get("arguments") or {}
                if name == "update_todolist":
                    return ActionDecision(action_type=ActionType.UPDATE_TODOLIST, action_name="create_todolist", parameters=params, reasoning="Need to (re)generate or adjust the plan", confidence=0.9)
                if name == "ask_user":
                    return ActionDecision(action_type=ActionType.ASK_USER, action_name="ask_user", parameters=params, reasoning="Need user input", confidence=0.9)
                if name == "complete":
                    return ActionDecision(action_type=ActionType.COMPLETE, action_name="complete", parameters=params, reasoning="Done", confidence=0.9)
                if name == "error_recovery":
                    return ActionDecision(action_type=ActionType.ERROR_RECOVERY, action_name="retry_failed", parameters=params, reasoning="Try to recover", confidence=0.7)
                if name == "delegate_to_agent":
                    agent_name = str(params.get("agent_name") or "").strip()
                    tool_name = f"agent_{self._normalize_name(agent_name)}" if agent_name else ""
                    tool_params = {
                        "task": params.get("task"),
                        "inputs": params.get("inputs") or {},
                        "shared_context": {
                            "session_id": self.session_id,
                            "version": int(self.context.get("version", 1)),
                            "facts": self.context.get("facts", {}),
                            "known_answers_text": self.context.get("known_answers_text", ""),
                        },
                        "allowed_tools": params.get("allowed_tools") or [],
                        "budget": params.get("budget") or {},
                    }
                    return ActionDecision(action_type=ActionType.TOOL_CALL, action_name=tool_name, parameters=tool_params, reasoning="Delegate to sub-agent tool", confidence=0.85)
                return ActionDecision(action_type=ActionType.TOOL_CALL, action_name=name, parameters=params, reasoning="Execute tool", confidence=0.8)
        except Exception as e:
            self.logger.warning("tool_calling_failed", error=str(e))

        # 2) Fallback: strukturierte Entscheidung
        prompt = (
            f"Context:\n{summary}\n\n"
            "Available actions: UPDATE_TODOLIST | TOOL_CALL | ASK_USER | ERROR_RECOVERY | COMPLETE\n"
            "Return the best next action."
        )
        try:
            dec = await self.llm.generate_structured_response(prompt, ActionDecision, system_prompt=self.final_system_prompt)
            # If we detect completion condition, force COMPLETE unless ASK_USER
            if dec.action_type != ActionType.ASK_USER and self._all_tasks_completed():
                return ActionDecision(action_type=ActionType.COMPLETE, action_name="complete", parameters={"summary": "All tasks completed."}, reasoning="All tasks done", confidence=0.95)
            if dec.action_type != ActionType.ASK_USER and not self.context.get("todolist_created"):
                return ActionDecision(action_type=ActionType.UPDATE_TODOLIST, action_name="create_todolist", parameters={}, reasoning="Bootstrap the plan", confidence=dec.confidence)
            return dec
        except Exception:
            return ActionDecision(action_type=ActionType.ERROR_RECOVERY, action_name="analyze", parameters={}, reasoning="Could not decide", confidence=0.5)

    async def _exec_with_retry(self, decision: ActionDecision, max_retries: int = 3) -> str:
        for attempt in range(1, max_retries + 1):
            try:
                res = await self._execute(decision.action_type, decision.action_name, decision.parameters)
                await self.feedback.collect_feedback(self.session_id, feedback_type=decision.action_type.value, success=True, details={"action": decision.action_name, "attempt": attempt})
                return res
            except Exception as e:
                self.logger.warning("action_failed", action=decision.action_name, attempt=attempt, error=str(e))
                if attempt == max_retries:
                    await self.feedback.collect_feedback(self.session_id, feedback_type=decision.action_type.value, success=False, details={"action": decision.action_name, "error": str(e)})
                    return f"Action '{decision.action_name}' failed after {max_retries} attempts: {e}"
                await asyncio.sleep(2 ** (attempt - 1))

    async def _execute(self, kind: ActionType, name: str, params: Dict[str, Any]) -> str:
        if kind == ActionType.UPDATE_TODOLIST:
            return await self._handle_todolist(name, params)
        if kind == ActionType.TOOL_CALL:
            return await self._handle_tool(name, params)
        # NOTE: delegate_to_agent will be routed as TOOL_CALL to agent_* tool names by LLM meta function
        if kind == ActionType.ASK_USER:
            return await self._handle_user_interaction(name, params)
        if kind == ActionType.ERROR_RECOVERY:
            return "Tried generic recovery (noop)."
        if kind == ActionType.COMPLETE:
            return params.get("summary", "OK")
        return f"Executed {kind.value}/{name}"

    # ===== TodoList handling (no LLM edits; Markdown is a view) =====
    async def _handle_todolist(self, action: str, params: Dict[str, Any]) -> str:
        act = action.lower().replace("-", "_").replace(" ", "_")
        if act in {"create_todolist", "create"}:
            # If a Todo List already exists, do NOT re-generate the plan to avoid overwriting state.
            # Only re-render the Markdown view from the authoritative in-memory state.
            if self.context.get("todolist_created"):
                self._render_markdown_view()
                return f"Todo List already exists at {self.context.get('todolist_file')}; skipped re-create."
            # Otherwise, create the initial plan
            await self._create_initial_plan(open_questions=[])
            return f"Todo List created at {self.context.get('todolist_file')}"
        if act == "update_item_status":
            # Back-compat: allow structured params {item_id,status,notes}
            item_id = params.get("item_id")
            status_text = str(params.get("status", "")).upper() or "PENDING"
            notes = params.get("notes")
            if not item_id:
                return "No item_id provided."
            updated = self._update_task_by_id(item_id, status_text, notes)
            if not updated:
                return f"Task '{item_id}' not found."
            self._render_markdown_view()
            return "Todo List updated."
        return f"Todo action '{action}' done"

    async def _handle_tool(self, tool_name: str, params: Dict[str, Any]) -> str:
        if not self.context.get("todolist_created"):
            await self._create_initial_plan(open_questions=[])
        norm = self._normalize_name(tool_name)

        # Instrumentation: execution time start (must be available for all paths)
        started_at = time.time()

        # Log which tool is about to be executed and acting agent identity
        try:
            agent_identity = str(self.context.get("agent_name") or "orchestrator").strip()
            self.logger.info(
                "tool_call_start",
                agent=agent_identity,
                tool=norm,
                raw_tool_name=str(tool_name),
                params_preview=list(sorted((params or {}).keys()))[:8],
            )
        except Exception:
            pass

        # Duplicate-Action Guard: skip if same executor/action/params caused no change previously
        try:
            exec_id = self._executor_id_for_tool(norm)
            action_name = self._action_for_tool(norm)
            params_hash = hashlib.md5(json.dumps(params, sort_keys=True, ensure_ascii=False).encode("utf-8")).hexdigest()
            sig = f"{exec_id}|{action_name}|{params_hash}"
            last_version = self._duplicate_action_cache.get(sig)
            if last_version is not None and int(self.context.get("version", 1)) == last_version:
                msg = await self._handle_user_interaction(
                    "ask_user",
                    {
                        "questions": [
                            "Gleiche Aktion mit identischen Parametern ohne Zustandsänderung erkannt. Zusätzliche Hinweise oder Strategie ändern?"
                        ],
                        "context": f"executor={exec_id}, action={action_name}",
                    },
                )
                # Ensure matching end-log even when we early-return due to duplicate guard
                try:
                    self.logger.info(
                        "tool_call_end",
                        tool=norm,
                        success=True,
                        duration_ms=int((time.time() - started_at) * 1000),
                    )
                except Exception:
                    pass
                return f"⏭️ Duplicate action guarded. {msg}"
        except Exception:
            pass

        # Deterministic status update: IN_PROGRESS only if not already COMPLETED
        try:
            idx_for_tool = self._find_task_for_tool(norm)
            if idx_for_tool is not None:
                cur_status = str(self._get_tasks()[idx_for_tool].get("status", "PENDING")).upper()
                if cur_status != TaskStatus.COMPLETED.value:
                    self._mark_task_for_tool(norm, TaskStatus.IN_PROGRESS)
                    self._render_markdown_view()
        except Exception:
            # Never fail the execution due to status pre-checks
            self._mark_task_for_tool(norm, TaskStatus.IN_PROGRESS)
            self._render_markdown_view()

        # Instrumentation: success/failure counters
        # Build tool params from decision without injecting non-serializable objects
        tool_params = dict(params or {})
        # If the target tool declares 'shared_context' in its schema, provide it by default
        try:
            spec = self.tool_index.get(self._normalize_name(tool_name))
            props = dict((spec.input_schema or {}).get("properties") or {}) if spec else {}
            if ("shared_context" in props):
                # Merge or create shared_context with tasks and target_task_id
                sc = dict(tool_params.get("shared_context") or {})
                sc.setdefault("session_id", self.session_id)
                sc.setdefault("version", int(self.context.get("version", 1)))
                sc.setdefault("facts", self.context.get("facts", {}))
                sc.setdefault("known_answers_text", self.context.get("known_answers_text", ""))
                sc.setdefault("user_inputs", self.context.get("user_inputs", []))
                # expose master tasks mapping and the current target task id for deterministic patching
                sc["tasks"] = self._get_tasks()
                try:
                    current_idx = self._find_task_for_tool(norm)
                    if current_idx is not None:
                        sc["target_task_id"] = str(self._get_tasks()[current_idx].get("id"))
                except Exception:
                    pass
                tool_params["shared_context"] = sc
        except Exception:
            pass
        before_version = int(self.context.get("version", 1))
        result = await execute_tool_by_name_from_index(self.tool_index, norm, tool_params)
        duration = time.time() - started_at
        try:
            tool_execution_time.labels(tool_name=norm).observe(duration)
            if bool(result.get("success")):
                tool_success_rate.labels(tool_name=norm).inc()
            else:
                tool_failure_rate.labels(tool_name=norm).inc()
        except Exception:
            # Metrics must never break the agent
            pass
        success = bool(result.get("success"))
        status = "COMPLETED" if success else "FAILED"
        result_text = json.dumps(result, ensure_ascii=False)

        # Log end of tool call with status
        try:
            self.logger.info(
                "tool_call_end",
                tool=norm,
                success=success,
                duration_ms=int(duration * 1000),
            )
        except Exception:
            pass

        # Special handling for sub-agent contract (generic; independent of tool name)
        # Bubble-up need_user_input without marking task failed
        if not success and result.get("need_user_input"):
            need = result.get("need_user_input") or {}
            self.context["pending_subagent_query"] = {
                "tool": tool_name,
                "state_token": result.get("state_token"),
                "questions": need.get("questions") or [],
                "context": need.get("context") or "",
                "task": params.get("task"),
                # propagate the sub-agent's identity if present in result or params
                "agent_name": str(need.get("agent_name") or params.get("agent_name") or tool_name),
            }
            _ = await self._handle_user_interaction(
                "ask_user",
                {
                    "questions": list(need.get("questions") or []),
                    "context": need.get("context") or "Sub-Agent requires additional information.",
                    "agent_name": str(need.get("agent_name") or params.get("agent_name") or tool_name),
                },
            )
            self._mark_task_for_tool(norm, TaskStatus.IN_PROGRESS, notes=result_text)
            self._render_markdown_view()
            # Ensure end log before returning to wait for user input
            try:
                self.logger.info(
                    "tool_call_end",
                    tool=norm,
                    success=False,
                    duration_ms=int((time.time() - started_at) * 1000),
                )
            except Exception:
                pass
            return "Sub-agent requires user input; awaiting response."

        # If sub-agent returned a patch, apply it
        patch = result.get("patch")
        if success and isinstance(patch, dict):
            merge_res = self._apply_patch(patch)
            result_text = json.dumps({"subagent_result": result, "merge": merge_res}, ensure_ascii=False)
            # Retry once on no-op applied=0: refresh capabilities, reconcile+revalidate, retry
            if merge_res.get("success") and int(merge_res.get("applied", 0)) == 0 and not merge_res.get("error"):
                # Retry once after reconciling tasks with current capabilities
                self.executor_index = self._build_executor_index()
                try:
                    tasks = self._reconcile_tasks_with_capabilities(self._get_tasks(), self.executor_index)
                    _ = self._validate_plan_against_capabilities(tasks, self.executor_index)
                    self._set_tasks(tasks)
                except Exception:
                    pass
                merge_res_retry = self._apply_patch(patch)
                result_text = json.dumps({"subagent_result": result, "merge": merge_res_retry}, ensure_ascii=False)
                if int(merge_res_retry.get("applied", 0)) == 0:
                    # Treat as no-op but successful sub-agent run; mark current task completed and continue
                    self._mark_task_for_tool(norm, TaskStatus.COMPLETED, notes=result_text)
                    self._render_markdown_view()
                    # Ensure we always emit a tool_call_end for this successful no-op path
                    try:
                        self.logger.info(
                            "tool_call_end",
                            tool=norm,
                            success=True,
                            duration_ms=int((time.time() - started_at) * 1000),
                        )
                    except Exception:
                        pass
                    return "No-Op patch: marked task completed"

            # On conflict (e.g., 409 or ownership issues), ask user instead of completing
            if (not bool(merge_res.get("success"))) or merge_res.get("error"):
                self._mark_task_for_tool(norm, TaskStatus.IN_PROGRESS, notes=result_text)
                self._render_markdown_view()
                msg = await self._handle_user_interaction(
                    "ask_user",
                    {
                        "questions": [
                            "Konflikt bei Sub-Agent-Patch (Version/Ownership). Wie fortfahren?"
                        ],
                        "context": str(merge_res),
                    },
                )
                # Emit end log for conflict path as well
                try:
                    self.logger.info(
                        "tool_call_end",
                        tool=norm,
                        success=False,
                        duration_ms=int((time.time() - started_at) * 1000),
                    )
                except Exception:
                    pass
                return f"⚠️ Patch-Konflikt: {msg}"
            self._mark_task_for_tool(norm, TaskStatus.COMPLETED, notes=result_text)
            self._render_markdown_view()
            return f"Applied sub-agent patch: {json.dumps(merge_res)}"

        # Deterministic status update after execution (generic tools)
        # Never downgrade a COMPLETED task back to IN_PROGRESS/FAILED due to noisy results
        try:
            idx_for_tool = self._find_task_for_tool(norm)
            if idx_for_tool is not None:
                cur_status = str(self._get_tasks()[idx_for_tool].get("status", "PENDING")).upper()
                next_status = TaskStatus.COMPLETED if success else TaskStatus.FAILED
                if cur_status == TaskStatus.COMPLETED.value and next_status != TaskStatus.COMPLETED:
                    # keep completed; just append notes
                    tasks = self._get_tasks()
                    if result_text:
                        tasks[idx_for_tool]["notes"] = result_text
                        self._set_tasks(tasks)
                        self._render_markdown_view()
                else:
                    self._mark_task_for_tool(norm, next_status, notes=result_text)
                    self._render_markdown_view()
            else:
                self._mark_task_for_tool(norm, TaskStatus.COMPLETED if success else TaskStatus.FAILED, notes=result_text)
                self._render_markdown_view()
        except Exception:
            self._mark_task_for_tool(norm, TaskStatus.COMPLETED if success else TaskStatus.FAILED, notes=result_text)
            self._render_markdown_view()

        # Generische Blocker-Hinweise ins Context
        err = (result.get("error") or "").strip()
        if err:
            self.context["blocker"] = {
                "message": err,
                "suggestion": "Gib korrigierte Parameter oder weitere Hinweise; ansonsten anderes Tool wählen."
            }

        # If all tasks are completed, hint to COMPLETE
        if self._all_tasks_completed():
            self.context["suggest_complete"] = True

        # Record duplicate-action guard signature if no version change occurred
        try:
            after_version = int(self.context.get("version", 1))
            if after_version == before_version:
                exec_id = self._executor_id_for_tool(norm)
                action_name = self._action_for_tool(norm)
                params_hash = hashlib.md5(json.dumps(params, sort_keys=True, ensure_ascii=False).encode("utf-8")).hexdigest()
                sig = f"{exec_id}|{action_name}|{params_hash}"
                self._duplicate_action_cache[sig] = after_version
        except Exception:
            pass

        return f"{tool_name} -> {json.dumps(result, indent=2)}"

    # ===== Sub-agent as ToolSpec factory =====
    def to_tool(
        self,
        *,
        name: str,
        description: str,
        allowed_tools: Optional[List[str]] = None,
        budget: Optional[Dict[str, Any]] = None,
        timeout: Optional[float] = 120.0,
        aliases: Optional[List[str]] = None,
        system_prompt_override: Optional[str] = None,  # deprecated: mapped to mission_override
        mission_override: Optional[str] = None,
    ) -> ToolSpec:
        """Return a ToolSpec that wraps this agent as a sub-agent tool.

        The wrapper runs a fresh ReActAgent instance with the same LLM and a tool whitelist.
        Inputs follow the sub-agent schema: task, inputs, shared_context, budget, resume_token, answers.
        """
        async def _subagent_tool(
            *,
            task: str,
            inputs: Optional[Dict[str, Any]] = None,
            shared_context: Optional[Dict[str, Any]] = None,
            budget: Optional[Dict[str, Any]] = None,
            resume_token: Optional[str] = None,
            answers: Optional[Dict[str, Any]] = None,
            **kwargs: Any,
        ) -> Dict[str, Any]:
            names = set(allowed_tools or [])
            tools_whitelist = [t for t in self.tools if not names or t.name in names]
            effective_mission = mission_override or system_prompt_override or None

            # --- snapshot original agent config
            _orig_tools = self.tools
            _orig_index = self.tool_index
            _orig_max_steps = self.max_steps
            _orig_mission = self.mission_text
            _orig_prompt = self.final_system_prompt
            _orig_session = self.session_id
            _orig_context = dict(self.context or {})

            sub_awaiting: Optional[Dict[str, Any]] = None
            sub_tasks_snapshot: List[Dict[str, Any]] = []
            try:
                # sandbox apply
                self.tools = tools_whitelist
                self.tool_index = build_tool_index(self.tools)
                if budget and "max_steps" in budget:
                    try:
                        self.max_steps = int(budget["max_steps"])
                    except Exception:
                        pass
                if effective_mission is not None:
                    self.mission_text = effective_mission
                    self.final_system_prompt = self._build_final_system_prompt()

                parent_sid = (shared_context or {}).get("session_id") or "no-session"
                self.session_id = f"{parent_sid}:sub:{name}"
                self.context = {
                    "user_request": task,
                    "known_answers_text": (shared_context or {}).get("known_answers_text", ""),
                    "user_inputs": (shared_context or {}).get("user_inputs", []),
                    "facts": (shared_context or {}).get("facts", {}),
                    "version": int((shared_context or {}).get("version", 1)),
                    "suppress_markdown": True,
                    "ephemeral_state": True,
                    "no_user_prompts": True,
                    "agent_name": name,
                }

                transcript: List[str] = []
                async for chunk in self.process_request(task, session_id=self.session_id):
                    transcript.append(chunk)
                # capture sub-agent state before rollback
                try:
                    sub_awaiting = self.context.get("awaiting_user_input")
                except Exception:
                    sub_awaiting = None
                try:
                    sub_tasks_snapshot = list(self.context.get("tasks", []))
                except Exception:
                    sub_tasks_snapshot = []
            finally:
                # rollback sandbox
                self.tools = _orig_tools
                self.tool_index = _orig_index
                self.max_steps = _orig_max_steps
                self.mission_text = _orig_mission
                self.final_system_prompt = _orig_prompt
                self.session_id = _orig_session
                self.context = _orig_context

            if sub_awaiting:
                return {
                    "success": False,
                    "need_user_input": sub_awaiting,
                    "state_token": "opaque",
                }

            # Build patch as update-only against master tasks to avoid duplicates and regressions
            master_tasks = list((shared_context or {}).get("tasks", []))
            target_task_id = str((shared_context or {}).get("target_task_id") or "").strip() or None
            wrapper_norm = name.strip().lower().replace("-", "_").replace(" ", "_")
            def _norm(s: str) -> str:
                return s.strip().lower().replace("-", "_").replace(" ", "_")
            def _find_master_task_id_by_tool(tool_name: str) -> Optional[str]:
                # 0) If orchestrator provided a target_task_id, use it deterministically
                if target_task_id:
                    return target_task_id
                norm = _norm(tool_name)
                # 1) Direct match on tool field
                for mt in master_tasks:
                    ttool = _norm(str(mt.get("tool") or ""))
                    if ttool and ttool == norm:
                        return str(mt.get("id"))
                # 2) Match on prefixed tool name (e.g., agent_git.create_repository)
                for mt in master_tasks:
                    ttool = _norm(str(mt.get("tool") or ""))
                    if ttool and ttool == f"{wrapper_norm}.{norm}":
                        return str(mt.get("id"))
                # 3) Fallback: match by executor_id + action
                for mt in master_tasks:
                    exec_id = _norm(str(mt.get("executor_id") or ""))
                    action = _norm(str(mt.get("action") or ""))
                    if action == norm and (not exec_id or exec_id == wrapper_norm):
                        return str(mt.get("id"))
                return None

            ops: list[dict] = []
            for t in sub_tasks_snapshot:
                tool_name = str(t.get("tool") or "").strip()
                status = str(t.get("status") or "").upper()
                if not tool_name:
                    continue
                if status not in {"IN_PROGRESS", "COMPLETED"}:
                    continue
                tid = _find_master_task_id_by_tool(tool_name)
                if not tid:
                    continue
                ops.append({
                    "op": "update",
                    "task_id": tid,
                    "fields": {"status": status}
                })

            patch = {
                "base_version": int((shared_context or {}).get("version", 1)),
                "agent_name": name,
                "ops": ops,
            }

            return {"success": True, "patch": patch, "result": {"transcript": "".join(transcript)}}

        schema = {
            "type": "object",
            "properties": {
                "task": {"type": "string"},
                "inputs": {"type": "object"},
                "shared_context": {"type": "object"},
                "budget": {"type": "object"},
                "resume_token": {"type": "string"},
                "answers": {"type": "object"},
            },
            "required": ["task"],
            "additionalProperties": True,
        }

        # Capabilities provider advertises sub-agent actions based on its internal tool whitelist
        def _caps_provider() -> Dict[str, Any]:
            names = set(allowed_tools or [])
            tools_whitelist = [t for t in self.tools if not names or t.name in names]
            actions = []
            for t in tools_whitelist:
                try:
                    actions.append({
                        "name": self._normalize_name(t.name),
                        "description": t.description,
                        "params_schema": t.input_schema or {"type": "object"},
                        "acceptance": None
                    })
                except Exception:
                    continue
            return {
                "executor_id": self._normalize_name(name),
                "label": description or name,
                "version": "cap-v1",
                "actions": actions,
            }

        return ToolSpec(
            name=name,
            description=description,
            input_schema=schema,
            output_schema={"type": "object"},
            func=_subagent_tool,
            is_async=True,
            timeout=timeout,
            aliases=aliases or [],
            capabilities_provider=_caps_provider,
        )

    # ===== Prompt Builder (new) =====
    def _build_final_system_prompt(self) -> str:
        """Compose the final system prompt in three sections or use legacy_full as requested.

        Modes:
          - compose (default): <GenericAgentSection> + <Mission> + <Tools>
          - legacy_full: use self.system_prompt_base 1:1 (back-compat path)
        """
        mode = str(self.prompt_overrides.get("mode", "compose")).strip().lower()
        if mode == "legacy_full":
            return self.system_prompt_base

        # Generic section
        generic = (self.system_prompt_base or DEFAULT_GENERIC_PROMPT).strip()

        # Mission section (always present, may be empty)
        mission = (self.mission_text or "").strip()

        # Tools section from ToolSpec list
        tool_lines: List[str] = []
        for spec in self.tools:
            req = list((spec.input_schema or {}).get("required", []))
            tool_lines.append(f"- {spec.name}: {spec.description}")
            if req:
                tool_lines.append(f"  required: {', '.join(req)}")

        parts = [
            "<GenericAgentSection>",
            generic,
            "</GenericAgentSection>",
            "",
            "<Mission>",
            mission,
            "</Mission>",
            "",
            "<Tools>",
            "\n".join(tool_lines) if tool_lines else "",
            "</Tools>",
        ]
        return "\n".join(parts).strip()

    # ===== ASK_USER =====
    async def _handle_user_interaction(self, action_name: str, params: Dict[str, Any]) -> str:
        # Block user prompts entirely for sub-agents when flagged
        if self.context.get("no_user_prompts"):
            return "Sub-agent: user prompts disabled"
        questions = params.get("questions") or []
        ctx = params.get("context") or ""
        try:
            # Improve observability: include agent identity and session in logs
            agent_name = str(self.context.get("agent_name") or params.get("agent_name") or "").strip() or None
            self.logger.info(
                "ask_user_triggered",
                agent=agent_name,
                session_id=self.session_id,
                action=action_name,
                num_questions=len(questions),
            )
        except Exception:
            pass
        self.context["awaiting_user_input"] = {
            "action": action_name,
            "questions": questions,
            "context": ctx,
            "requested_at": datetime.now().isoformat()
        }
        await self._save()
        lines = ["User input needed:", f"Context: {ctx}"]
        if questions:
            lines.append("Questions:")
            for i, q in enumerate(questions, 1):
                lines.append(f"  {i}. {q}")
        lines.append("\nBitte antworte frei-form.")
        return "\n".join(lines)

    # ===== helpers =====
    def _store_user_reply(self, msg: str):
        awaiting = self.context.get("awaiting_user_input") or {}
        entry = {
            "answer": msg,
            "for_action": awaiting.get("action"),
            "questions": awaiting.get("questions", []),
            "provided_at": datetime.now().isoformat(),
        }
        self.context.setdefault("user_inputs", []).append(entry)
        self.context.pop("awaiting_user_input", None)

        # CHANGED: keep known answers text blob for prompts
        answers_text = self.context.get("known_answers_text", "")
        answers_text += f"\n- {msg}"
        self.context["known_answers_text"] = answers_text.strip()

        # CHANGED: simple fact extraction (kebab-case -> project_name)
        m = re.search(r"\b([a-z0-9]+(?:-[a-z0-9]+)+)\b", msg)
        if m:
            self.context.setdefault("facts", {})["project_name"] = m.group(1)

    def _summary_for_llm(self) -> str:
        lines = [
            f"Session: {self.session_id}",
            f"User Request: {self.context.get('user_request','')}",
            f"Step: {self.step}/{self.max_steps}",
        ]
        if self.context.get("recent_user_message"):
            lines.append(f"Recent User Message: {self.context['recent_user_message']}")
        tasks = self.context.get("tasks")
        if tasks:
            lines.append(f"Tasks: {len(tasks)} total")
            # Show up to first 5 tasks with minimal info for planning
            for t in tasks[:5]:
                tid = t.get("id")
                title = t.get("title")
                status = t.get("status")
                tool = t.get("tool") or "-"
                lines.append(f"  - {tid}: {title} [{status}] (tool: {tool})")
        else:
            lines.append("Tasks: none yet")
        if self.context.get("todolist_file"):
            lines.append(f"Todo List File: {self.context['todolist_file']}")
        if self.react_history:
            lines.append("Recent Actions:")
            for a in self.react_history[-8:]:
                lines.append("  - " + a)
        if self.context.get("blocker"):
            lines.append(f"Blocker: {self.context['blocker']}")
        if self.context.get("known_answers_text"):  # CHANGED: add known answers to summary context
            lines.append("Known Answers:")
            lines.append(self.context["known_answers_text"])
        if self.context.get("facts"):  # CHANGED: add parsed facts
            lines.append(f"Facts: {self.context['facts']}")
        return "\n".join(lines)

    async def _update_context(self, decision: ActionDecision, observation: str):
        self.context["last_action"] = {
            "type": decision.action_type.value, "name": decision.action_name,
            "result": observation, "timestamp": datetime.now().isoformat()
        }
        self.react_history.append(f"{decision.action_name} -> {observation[:100]}")
        if len(self.react_history) > 32:
            self.react_history = self.react_history[-32:]

    def _record_and_check_loop(self, decision: ActionDecision, observation: str, *, window_size: int = 3) -> bool:
        """
        Record a normalized signature of the (action, observation) pair and
        return True if the last `window_size` signatures are identical.
        """
        sig = self._signature_for_loop(decision, observation)
        self._loop_signatures.append(sig)
        if len(self._loop_signatures) > window_size:
            self._loop_signatures = self._loop_signatures[-window_size:]
        if len(self._loop_signatures) == window_size and len(set(self._loop_signatures)) == 1:
            self.context["loop_guard_cooldown"] = int(self.context.get("loop_guard_cooldown", 0)) + 1
            return True
        return False

    def _signature_for_loop(self, decision: ActionDecision, observation: str) -> str:
        # Normalize observation to reduce noise and truncate to a stable prefix
        obs = observation.strip().lower()
        obs = re.sub(r"\s+", " ", obs)
        obs = obs[:160]
        return f"{decision.action_type.value}|{decision.action_name.lower()}|{obs}"

    async def _save(self):
        if self.context.get("ephemeral_state"):
            return
        await self.state.save_state(self.session_id, {
            "context": self.context,
            "react_history": self.react_history,
            "step": self.step,
        })

    def _restore(self, s: Dict[str, Any]):
        self.context = s.get("context", {})
        self.react_history = s.get("react_history", [])
        self.step = s.get("step", 0)

    def _compose_system_prompt(self) -> str:
        lines = [self.system_prompt_base, "\n## TOOLS (dynamic)\n"]
        for spec in self.tools:
            req = list((spec.input_schema or {}).get("required", []))
            lines.append(f"- {spec.name}: {spec.description}")
            if req: lines.append(f"  required: {', '.join(req)}")
        lines.append(
            "\nUsage rules:\n"
            "- Use only listed tools.\n"
            "- First, build a Todo List (plan) as structured JSON; minor clarifications go into 'Open Questions'.\n"
            "- After each tool run, status is updated deterministically; Markdown is a view only.\n"
            "- If a blocking error occurs, ASK_USER with concrete suggestions."
        )
        return "\n".join(lines)

    # ===== Structured Task helpers =====
    def _normalize_name(self, name: str) -> str:
        return name.strip().lower().replace("-", "_").replace(" ", "_")

    # ===== Capabilities & Planning helpers =====
    def _build_executor_index(self) -> Dict[str, Dict[str, Any]]:
        """Build executor capabilities from registered tools.

        For tools without explicit capabilities, synthesize a single action using the tool name.
        Returns a map executor_id -> capabilities dict with keys: executor_id, label, version, actions[].
        """
        index: Dict[str, Dict[str, Any]] = {}
        for spec in self.tools:
            try:
                cap: Dict[str, Any]
                # If tool provides its own capabilities
                if getattr(spec, "capabilities_provider", None):
                    cap = spec.capabilities_provider()  # type: ignore[misc]
                    eid = str(cap.get("executor_id") or self._normalize_name(spec.name))
                    cap["executor_id"] = eid
                    index[eid] = cap
                else:
                    eid = self._normalize_name(spec.name)
                    cap = {
                        "executor_id": eid,
                        "label": spec.name,
                        "version": "cap-v1",
                        "actions": [
                            {
                                "name": self._normalize_name(spec.name),
                                "description": getattr(spec, "description", spec.name),
                                "params_schema": spec.input_schema or {"type": "object"},
                                "acceptance": None,
                            }
                        ],
                    }
                    index[eid] = cap
                try:
                    # Structured capability log per executor
                    self.logger.info(
                        "executor_capabilities",
                        executor_id=index[eid]["executor_id"],
                        label=index[eid].get("label"),
                        version=index[eid].get("version"),
                        actions=[a.get("name") for a in index[eid].get("actions", [])],
                    )
                except Exception:
                    pass
            except Exception:
                continue
        # Keep in context for prompts/visibility
        self.context["executor_index"] = index
        return index

    def _executor_id_for_tool(self, tool_norm: str) -> str:
        # Prefer matching synthesized eid equal to tool name
        eid = self._normalize_name(tool_norm)
        if eid in self.executor_index:
            return eid
        # Fallback: first executor containing an action matching tool name
        for k, cap in (self.executor_index or {}).items():
            for a in cap.get("actions", []) or []:
                if self._normalize_name(a.get("name", "")) == tool_norm:
                    return k
        return eid

    def _action_for_tool(self, tool_norm: str) -> str:
        # Default action name equals normalized tool name for synthesized capabilities
        return self._normalize_name(tool_norm)

    def _validate_plan_against_capabilities(self, tasks: List[Dict[str, Any]], executor_index: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Validate tasks against capability index. Marks invalid tasks with status ERROR and notes.
        Returns a report dict; tasks are mutated in place with notes when invalid.
        """
        report = {"invalid": 0}
        for t in tasks:
            eid = t.get("executor_id")
            act = t.get("action")
            if not eid or eid not in executor_index:
                t["status"] = TaskStatus.FAILED.value
                t["notes"] = (t.get("notes") or "") + "; invalid executor_id"
                report["invalid"] += 1
                continue
            actions = [a.get("name") for a in executor_index[eid].get("actions", [])]
            if not act or self._normalize_name(act) not in [self._normalize_name(a) for a in actions]:
                t["status"] = TaskStatus.FAILED.value
                t["notes"] = (t.get("notes") or "") + "; invalid action"
                report["invalid"] += 1
        return report

    def _reconcile_tasks_with_capabilities(self, tasks: List[Dict[str, Any]], executor_index: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Fill missing executor_id/action when determinable without domain knowledge."""
        # If only one executor, set missing executor_id
        visible = list(executor_index.keys())
        single_executor = visible[0] if len(visible) == 1 else None
        for t in tasks:
            # Derive executor_id from tool name if missing
            if not t.get("executor_id"):
                tool = (t.get("tool") or "").strip()
                if tool:
                    t["executor_id"] = self._executor_id_for_tool(self._normalize_name(tool))
                elif single_executor:
                    t["executor_id"] = single_executor
            # Derive action if missing or invalid and executor has single action
            eid = t.get("executor_id")
            if eid and eid in executor_index:
                actions = executor_index[eid].get("actions", [])
                if not t.get("action"):
                    if len(actions) == 1:
                        t["action"] = actions[0].get("name")
                    else:
                        # Try to match tool to action
                        tool = (t.get("tool") or "").strip()
                        cand = self._normalize_name(tool) if tool else None
                        for a in actions:
                            if cand and self._normalize_name(a.get("name", "")) == cand:
                                t["action"] = a.get("name")
                                break
        return tasks

    def _prune_invalid_and_duplicate_tasks(self, tasks: List[Dict[str, Any]], executor_index: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove tasks that reference non-existent executors/actions and collapse duplicates.

        Duplicate criteria: same normalized (executor_id, action, json.dumps(params, sort_keys=True)).
        """
        pruned: List[Dict[str, Any]] = []
        seen: set[str] = set()
        for t in tasks:
            eid = t.get("executor_id")
            act = t.get("action")
            if not eid or eid not in executor_index:
                continue
            actions = [self._normalize_name(a.get("name")) for a in executor_index[eid].get("actions", [])]
            if not act or self._normalize_name(act) not in actions:
                continue
            try:
                params = t.get("params") or {}
                key = json.dumps({
                    "eid": self._normalize_name(eid),
                    "act": self._normalize_name(act),
                    "params": params,
                }, sort_keys=True, ensure_ascii=False)
            except Exception:
                key = f"{self._normalize_name(eid)}|{self._normalize_name(act)}"
            if key in seen:
                continue
            seen.add(key)
            pruned.append(t)
        # reassign stable ids t1..tn if missing or duplicate ids present
        assigned_ids: set[str] = set()
        for i, t in enumerate(pruned, start=1):
            tid = str(t.get("id") or "").strip()
            if not tid or tid in assigned_ids:
                t["id"] = f"t{i}"
            assigned_ids.add(str(t.get("id")))
        return pruned

    def _get_tasks(self) -> List[Dict[str, Any]]:
        return list(self.context.get("tasks", []))

    def _set_tasks(self, tasks: List[Dict[str, Any]]):
        self.context["tasks"] = tasks

    def _render_markdown_view(self):
        # Allow disabling Markdown rendering (e.g., for sub-agents)
        if self.context.get("suppress_markdown"):
            return
        try:
            # Ensure we never downgrade statuses while rendering; normalize first
            tasks = self._get_tasks()
            for t in tasks:
                t["status"] = self._normalize_status_value(t.get("status"))
            path = render_todolist_markdown(
                tasks=tasks,
                open_questions=self.context.get("open_questions", []),
                session_id=self.session_id,
            )
            self.context["todolist_file"] = path
        except Exception:
            # Rendering must not break execution
            pass

    def _find_task_for_tool(self, tool_norm: str) -> Optional[int]:
        tasks = self._get_tasks()
        # Prefer first task matching tool name that is not COMPLETED
        for idx, t in enumerate(tasks):
            t_tool = str(t.get("tool") or "").strip()
            status = str(t.get("status", "PENDING")).upper()
            if t_tool and self._normalize_name(t_tool) == tool_norm and status != TaskStatus.COMPLETED.value:
                return idx
        # Fallback: first PENDING/IN_PROGRESS task
        for idx, t in enumerate(tasks):
            status = str(t.get("status", "PENDING")).upper()
            if status in {TaskStatus.PENDING.value, TaskStatus.IN_PROGRESS.value}:
                return idx
        return None

    def _mark_task_for_tool(self, tool_norm: str, status: TaskStatus, *, notes: str | None = None):
        idx = self._find_task_for_tool(tool_norm)
        if idx is None:
            return
        tasks = self._get_tasks()
        tasks[idx]["status"] = status.value
        if notes:
            tasks[idx]["notes"] = notes
        self._set_tasks(tasks)

    def _update_task_by_id(self, item_id: str, status_text: str, notes: Optional[str]) -> bool:
        tasks = self._get_tasks()
        for i, t in enumerate(tasks):
            if str(t.get("id")) == str(item_id):
                tasks[i]["status"] = status_text
                if notes:
                    tasks[i]["notes"] = notes
                self._set_tasks(tasks)
                return True
        return False

    # ===== Patch Engine (Sub-Agent -> Orchestrator) =====
    def _find_task_index_by_id(self, task_id: str) -> Optional[int]:
        tasks = self._get_tasks()
        for i, t in enumerate(tasks):
            if str(t.get("id")) == str(task_id):
                return i
        return None

    def _normalize_status_value(self, value: Any) -> str:
        """Normalize status values to uppercase strings like 'PENDING'/'COMPLETED'."""
        try:
            if isinstance(value, TaskStatus):
                return value.value
            s = str(value).strip()
            if "." in s and s.upper().startswith("TASKSTATUS"):
                s = s.split(".")[-1]
            return s.upper() or TaskStatus.PENDING.value
        except Exception:
            return TaskStatus.PENDING.value

    def _all_tasks_completed(self) -> bool:
        tasks = self._get_tasks()
        if not tasks:
            return False
        for t in tasks:
            if str(t.get("status", "")).upper() != TaskStatus.COMPLETED.value:
                return False
        return True

    def _apply_patch(self, patch: Dict[str, Any]) -> Dict[str, Any]:
        """Apply a sub-agent patch with version/ownership guards.

        Patch format:
          {"base_version": int, "agent_name": str, "ops": [ ... ]}

        Supported ops:
          - {"op":"update", "task_id": str, "fields": {...}}
          - {"op":"add", "task": {...}}
          - {"op":"add_subtask", "parent_id": str, "task": {...}}
          - {"op":"link_dep", "task_id": str, "depends_on": [str,...]}
          - {"op":"unlink_dep", "task_id": str, "depends_on": [str,...]}
          - {"op":"set_status", "task_id": str, "value": "COMPLETED"}
          - {"op":"set_field", "task_id": str, "path": "notes", "value": "..."}
          - {"op":"append_task", "task": Task}
          - {"op":"remove_task", "task_id": str}
          - {"op":"replace_task", "task_id": str, "task": Task}
        """
        current_version = int(self.context.get("version", 1))
        base_version = int(patch.get("base_version") or patch.get("applied_to_plan_version") or current_version)
        agent_name = str(patch.get("agent_name") or "").strip() or None

        if base_version != current_version:
            return {"success": False, "error": f"409 Conflict: base_version {base_version} != current {current_version}"}

        tasks = self._get_tasks()
        applied: int = 0
        denied: list[str] = []

        for op in patch.get("ops", []) or []:
            try:
                kind = str(op.get("op") or "").lower()
                if kind == "update":
                    tid = op.get("task_id")
                    fields = dict(op.get("fields") or {})
                    idx = self._find_task_index_by_id(str(tid)) if tid else None
                    if idx is None:
                        denied.append(f"update:{tid}:not_found")
                        continue
                    owner = tasks[idx].get("owner_agent")
                    if owner and agent_name and owner != agent_name:
                        denied.append(f"update:{tid}:owner_mismatch")
                        continue
                    # allow updating standard fields only; ignore unknowns
                    for k in ["title", "description", "tool", "params", "status", "notes", "priority"]:
                        if k in fields:
                            if k == "status":
                                tasks[idx][k] = self._normalize_status_value(fields[k])
                            else:
                                tasks[idx][k] = fields[k]
                    applied += 1

                elif kind == "annotate":
                    tid = op.get("task_id")
                    note = str(op.get("note") or "").strip()
                    idx = self._find_task_index_by_id(str(tid)) if tid else None
                    if idx is None:
                        denied.append(f"annotate:{tid}:not_found")
                        continue
                    owner = tasks[idx].get("owner_agent")
                    if owner and agent_name and owner != agent_name:
                        denied.append(f"annotate:{tid}:owner_mismatch")
                        continue
                    cur = str(tasks[idx].get("notes") or "").strip()
                    tasks[idx]["notes"] = (cur + ("; " if cur and note else "") + note).strip()
                    applied += 1

                elif kind == "add":
                    t = dict(op.get("task") or {})
                    if agent_name:
                        t.setdefault("owner_agent", agent_name)
                    # default fields
                    t["status"] = self._normalize_status_value(t.get("status", TaskStatus.PENDING.value))
                    t.setdefault("id", f"t{len(tasks)+1}")
                    # de-duplicate by id: if exists, merge instead of append
                    exist_idx = self._find_task_index_by_id(str(t.get("id")))
                    if exist_idx is not None:
                        owner = tasks[exist_idx].get("owner_agent")
                        if owner and agent_name and owner != agent_name:
                            denied.append(f"add:{t.get('id')}:owner_mismatch")
                        else:
                            for k in ["title", "description", "tool", "params", "status", "notes", "priority", "owner_agent", "depends_on"]:
                                if k in t:
                                    if k == "status":
                                        tasks[exist_idx][k] = self._normalize_status_value(t[k])
                                    else:
                                        tasks[exist_idx][k] = t[k]
                            applied += 1
                    else:
                        tasks.append(t)
                        applied += 1

                elif kind == "add_subtask":
                    parent_id = op.get("parent_id")
                    t = dict(op.get("task") or {})
                    if agent_name:
                        t.setdefault("owner_agent", agent_name)
                    # mark relation in notes and depends_on
                    notes = (t.get("notes") or "").strip()
                    rel_note = f"subtask_of:{parent_id}"
                    t["notes"] = (notes + ("; " if notes else "") + rel_note).strip()
                    deps = list(t.get("depends_on") or [])
                    if parent_id:
                        deps.append(str(parent_id))
                    t["depends_on"] = sorted(set(map(str, deps)))
                    t["status"] = self._normalize_status_value(t.get("status", TaskStatus.PENDING.value))
                    t.setdefault("id", f"t{len(tasks)+1}")
                    # de-duplicate by id
                    exist_idx = self._find_task_index_by_id(str(t.get("id")))
                    if exist_idx is not None:
                        owner = tasks[exist_idx].get("owner_agent")
                        if owner and agent_name and owner != agent_name:
                            denied.append(f"add_subtask:{t.get('id')}:owner_mismatch")
                        else:
                            for k in ["title", "description", "tool", "params", "status", "notes", "priority", "owner_agent", "depends_on"]:
                                if k in t:
                                    if k == "status":
                                        tasks[exist_idx][k] = self._normalize_status_value(t[k])
                                    else:
                                        tasks[exist_idx][k] = t[k]
                            applied += 1
                    else:
                        tasks.append(t)
                        applied += 1

                elif kind == "link_dep":
                    tid = op.get("task_id")
                    links = [str(x) for x in (op.get("depends_on") or [])]
                    idx = self._find_task_index_by_id(str(tid)) if tid else None
                    if idx is None:
                        denied.append(f"link_dep:{tid}:not_found")
                        continue
                    owner = tasks[idx].get("owner_agent")
                    if owner and agent_name and owner != agent_name:
                        denied.append(f"link_dep:{tid}:owner_mismatch")
                        continue
                    cur = set(map(str, tasks[idx].get("depends_on") or []))
                    cur.update(links)
                    tasks[idx]["depends_on"] = sorted(cur)
                    applied += 1

                elif kind == "unlink_dep":
                    tid = op.get("task_id")
                    unlink = set(map(str, (op.get("depends_on") or [])))
                elif kind == "set_status":
                    tid = op.get("task_id")
                    value = op.get("value")
                    idx = self._find_task_index_by_id(str(tid)) if tid else None
                    if idx is None:
                        denied.append(f"set_status:{tid}:not_found")
                        continue
                    owner = tasks[idx].get("owner_agent")
                    if owner and agent_name and owner != agent_name:
                        denied.append(f"set_status:{tid}:owner_mismatch")
                        continue
                    tasks[idx]["status"] = self._normalize_status_value(value)
                    applied += 1

                elif kind == "set_field":
                    tid = op.get("task_id")
                    path = str(op.get("path") or "").strip()
                    value = op.get("value")
                    idx = self._find_task_index_by_id(str(tid)) if tid else None
                    if idx is None or not path:
                        denied.append(f"set_field:{tid}:invalid")
                        continue
                    owner = tasks[idx].get("owner_agent")
                    if owner and agent_name and owner != agent_name:
                        denied.append(f"set_field:{tid}:owner_mismatch")
                        continue
                    # Only allow top-level safe fields
                    if path in {"title","description","tool","params","status","notes","priority","executor_id","action"}:
                        if path == "status":
                            tasks[idx][path] = self._normalize_status_value(value)
                        else:
                            tasks[idx][path] = value
                        applied += 1
                    else:
                        denied.append(f"set_field:{tid}:path_not_allowed")

                elif kind == "append_task":
                    t = dict(op.get("task") or {})
                    if agent_name:
                        t.setdefault("owner_agent", agent_name)
                    t["status"] = self._normalize_status_value(t.get("status", TaskStatus.PENDING.value))
                    t.setdefault("id", f"t{len(tasks)+1}")
                    exist_idx = self._find_task_index_by_id(str(t.get("id")))
                    if exist_idx is not None:
                        denied.append(f"append_task:{t.get('id')}:exists")
                    else:
                        tasks.append(t)
                        applied += 1

                elif kind == "remove_task":
                    tid = op.get("task_id")
                    idx = self._find_task_index_by_id(str(tid)) if tid else None
                    if idx is None:
                        denied.append(f"remove_task:{tid}:not_found")
                        continue
                    owner = tasks[idx].get("owner_agent")
                    if owner and agent_name and owner != agent_name:
                        denied.append(f"remove_task:{tid}:owner_mismatch")
                        continue
                    tasks.pop(idx)
                    applied += 1

                elif kind == "replace_task":
                    tid = op.get("task_id")
                    new_t = dict(op.get("task") or {})
                    idx = self._find_task_index_by_id(str(tid)) if tid else None
                    if idx is None:
                        denied.append(f"replace_task:{tid}:not_found")
                        continue
                    owner = tasks[idx].get("owner_agent")
                    if owner and agent_name and owner != agent_name:
                        denied.append(f"replace_task:{tid}:owner_mismatch")
                        continue
                    # preserve id unless explicitly set
                    new_t.setdefault("id", tasks[idx].get("id"))
                    new_t["status"] = self._normalize_status_value(new_t.get("status", TaskStatus.PENDING.value))
                    tasks[idx] = new_t
                    applied += 1
                    idx = self._find_task_index_by_id(str(tid)) if tid else None
                    if idx is None:
                        denied.append(f"unlink_dep:{tid}:not_found")
                        continue
                    owner = tasks[idx].get("owner_agent")
                    if owner and agent_name and owner != agent_name:
                        denied.append(f"unlink_dep:{tid}:owner_mismatch")
                        continue
                    cur = set(map(str, tasks[idx].get("depends_on") or []))
                    tasks[idx]["depends_on"] = sorted(cur - unlink)
                    applied += 1

                else:
                    denied.append(f"{kind}:unsupported")
            except Exception as e:
                denied.append(f"{op.get('op')}:error:{e}")

        # Persist changes if any op applied
        if applied > 0:
            self._set_tasks(tasks)
            self.context["version"] = int(self.context.get("version", current_version)) + 1
            self._render_markdown_view()

        return {"success": True, "applied": applied, "denied": denied, "new_version": int(self.context.get("version", current_version))}




// Relative Path: prototype\feedback_collector.py
# ==================== FEEDBACK COLLECTOR ====================

from collections import defaultdict
from datetime import datetime
import json
from pathlib import Path
from typing import Dict

import structlog


class FeedbackCollector:
    """Collects and stores user feedback for continuous improvement"""
    
    def __init__(self, feedback_dir: str = "./feedback"):
        self.feedback_dir = Path(feedback_dir)
        self.feedback_dir.mkdir(exist_ok=True)
        self.feedback_buffer = []
        self.logger = structlog.get_logger()
    
    async def collect_feedback(self, session_id: str, feedback_type: str, 
                              success: bool, details: Dict) -> None:
        """Collect feedback asynchronously"""
        feedback_entry = {
            'session_id': session_id,
            'timestamp': datetime.now().isoformat(),
            'type': feedback_type,
            'success': success,
            'details': details
        }
        
        self.feedback_buffer.append(feedback_entry)
        
        # Flush buffer if it gets too large
        if len(self.feedback_buffer) >= 100:
            await self.flush_feedback()
    
    async def flush_feedback(self) -> None:
        """Write feedback buffer to disk"""
        if not self.feedback_buffer:
            return
        
        filename = f"feedback_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = self.feedback_dir / filename
        
        try:
            import aiofiles
            async with aiofiles.open(filepath, 'w') as f:
                await f.write(json.dumps(self.feedback_buffer, indent=2))
            
            self.logger.info("feedback_flushed", count=len(self.feedback_buffer))
            self.feedback_buffer.clear()
            
        except Exception as e:
            self.logger.error("feedback_flush_failed", error=str(e))
    
    def analyze_feedback(self, days: int = 30) -> Dict:
        """Analyze recent feedback for patterns"""
        analysis = {
            'total_feedback': 0,
            'success_rate': 0,
            'common_failures': defaultdict(int),
            'tool_performance': defaultdict(lambda: {'success': 0, 'failure': 0})
        }
        
        cutoff_date = datetime.now().timestamp() - (days * 24 * 60 * 60)
        
        for feedback_file in self.feedback_dir.glob("*.json"):
            if feedback_file.stat().st_mtime < cutoff_date:
                continue
            
            with open(feedback_file) as f:
                feedback_data = json.load(f)
                
                for entry in feedback_data:
                    analysis['total_feedback'] += 1
                    
                    if entry['success']:
                        analysis['success_rate'] += 1
                    
                    if not entry['success'] and 'error' in entry['details']:
                        analysis['common_failures'][entry['details']['error']] += 1
                    
                    if 'tool_name' in entry['details']:
                        tool = entry['details']['tool_name']
                        if entry['success']:
                            analysis['tool_performance'][tool]['success'] += 1
                        else:
                            analysis['tool_performance'][tool]['failure'] += 1
        
        if analysis['total_feedback'] > 0:
            analysis['success_rate'] = analysis['success_rate'] / analysis['total_feedback']
        
        return analysis




// Relative Path: prototype\llm_provider.py
# ==================== LLM ABSTRACTION ====================

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

from pydantic import BaseModel
import structlog


class LLMProvider(ABC):
    """Abstract base class for LLM providers"""
    
    @abstractmethod
    async def generate_response(self, prompt: str, **kwargs) -> str:
        pass
    
    @abstractmethod
    async def generate_structured_response(self, prompt: str, response_model: BaseModel, **kwargs) -> BaseModel:
        pass

    @abstractmethod
    async def call_tools(self, *, system_prompt: str, messages: List[Dict[str, str]], tools: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Invoke vendor tool-calling if available.

        Returns a dict like {"name": str, "arguments": dict} when a tool is selected, otherwise None.
        """
        pass

class OpenAIProvider(LLMProvider):
    """OpenAI/GPT implementation using official OpenAI SDK (no langchain)."""
    
    #def __init__(self, api_key: str, model: str = "gpt-5-mini-2025-08-07", temperature: float = 0.1):
    def __init__(self, api_key: str, model: str = "gpt-4.1", temperature: float = 0.1):
        from openai import AsyncOpenAI
        
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        self.temperature = temperature
        self.logger = structlog.get_logger()
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        system_prompt = kwargs.get('system_prompt', '')
        try:
            completion = await self.client.chat.completions.create(
                model=self.model,
                # reasoning_effort="minimal",
                # verbosity="low",  
                temperature=self.temperature,                
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt},
                ],
            )
            return completion.choices[0].message.content or ""
        except Exception as e:
            self.logger.error("llm_generation_failed", error=str(e))
            raise
    
    async def generate_structured_response(self, prompt: str, response_model: BaseModel, **kwargs) -> BaseModel:
        import json as _json
        
        # Support both model class and instance
        model_cls = response_model if isinstance(response_model, type) else response_model.__class__
        if hasattr(model_cls, 'model_json_schema'):
            schema = model_cls.model_json_schema()
        else:
            # Fallback for environments exposing .schema()
            schema = model_cls.schema()  # type: ignore[attr-defined]
        
        # 1) Preferred path: Vendor function-calling to force a validated JSON return
        # We expose a single function whose parameters are exactly the target schema.
        try:
            completion = await self.client.chat.completions.create(
                model=self.model,
                # reasoning_effort="minimal",
                # verbosity="low",
                temperature=self.temperature,
                messages=[
                    {"role": "system", "content": kwargs.get('system_prompt', '')},
                    {"role": "user", "content": prompt},
                ],
                tools=[
                    {
                        "type": "function",
                        "function": {
                            "name": "return_structured",
                            "description": f"Return a JSON object that matches the provided schema for {model_cls.__name__}.",
                            "parameters": schema,
                        },
                    }
                ],
                tool_choice={"type": "function", "function": {"name": "return_structured"}},
            )
            choice = completion.choices[0]
            tool_calls = getattr(choice.message, "tool_calls", None)
            if tool_calls and len(tool_calls) > 0:
                args_str = tool_calls[0].function.arguments
                data = _json.loads(args_str)
                return model_cls(**data)
        except Exception as e:
            # Fall through to schema-guided JSON prompting
            self.logger.warning("openai_function_calling_failed", error=str(e), model=model_cls.__name__)
        
        # 2) Fallback: Schema-guided prompting with plain JSON response
        structured_prompt = f"""{prompt}
\nRespond with valid JSON matching this schema:
{_json.dumps(schema, indent=2)}
\nJSON Response:"""
        try:
            text = await self.generate_response(structured_prompt, **kwargs)
            json_str = text.strip()
            if json_str.startswith("```json"):
                json_str = json_str[7:]
            if json_str.endswith("```"):
                json_str = json_str[:-3]
            data = _json.loads(json_str.strip())
            return model_cls(**data)
        except Exception as e:
            self.logger.error("structured_generation_failed", error=str(e), model=model_cls.__name__)
            raise

    async def call_tools(self, *, system_prompt: str, messages: List[Dict[str, str]], tools: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Call OpenAI function/tool calling and return the chosen tool call if any."""
        try:
            completion = await self.client.chat.completions.create(
                model=self.model,
                # reasoning_effort="minimal",
                # verbosity="low",
                temperature=self.temperature,
                messages=[{"role": "system", "content": system_prompt}] + messages,
                tools=tools,
                tool_choice="auto",
            )
            choice = completion.choices[0]
            tool_calls = getattr(choice.message, "tool_calls", None)
            if tool_calls and len(tool_calls) > 0:
                tool_call = tool_calls[0]
                name = tool_call.function.name
                import json as _json
                args = {}
                try:
                    args = _json.loads(tool_call.function.arguments or "{}")
                except Exception:
                    args = {}
                return {"name": name, "arguments": args}
            return None
        except Exception as e:
            self.logger.warning("openai_call_tools_failed", error=str(e))
            return None

class AnthropicProvider(LLMProvider):
    """Anthropic/Claude implementation"""
    
    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229", temperature: float = 0.1):
        import anthropic
        self.client = anthropic.AsyncAnthropic(api_key=api_key)
        self.model = model
        self.temperature = temperature
        self.logger = structlog.get_logger()
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        try:
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=4096,
                temperature=self.temperature,
                system=kwargs.get('system_prompt', ''),
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        except Exception as e:
            self.logger.error("anthropic_generation_failed", error=str(e))
            raise
    
    async def generate_structured_response(self, prompt: str, response_model: BaseModel, **kwargs) -> BaseModel:
        import json
        
        model_cls = response_model if isinstance(response_model, type) else response_model.__class__
        schema = model_cls.model_json_schema() if hasattr(model_cls, "model_json_schema") else model_cls.schema()  # type: ignore[attr-defined]
        structured_prompt = f"""{prompt}

Respond with valid JSON matching this schema:
{json.dumps(schema, indent=2)}

JSON Response:"""
        
        try:
            response = await self.generate_response(structured_prompt, **kwargs)
            # Extract JSON from response
            json_str = response.strip()
            if json_str.startswith("```json"):
                json_str = json_str[7:]
            if json_str.endswith("```"):
                json_str = json_str[:-3]
            
            data = json.loads(json_str.strip())
            return model_cls(**data)
        except Exception as e:
            self.logger.error("anthropic_structured_failed", error=str(e))
            raise

    async def call_tools(self, *, system_prompt: str, messages: List[Dict[str, str]], tools: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Anthropic provider does not use function calling here; return None."""
        return None




// Relative Path: prototype\prompt.py
"""System prompts used by the IDP Copilot.

Phase 4 introduces modular prompt files. These constants remain as defaults.
"""

from pathlib import Path


def _load_prompt_file(filename: str, default_text: str) -> str:
    """Load a prompt text from capstone/prototype/prompts/<filename> if present.

    Falls back to provided default_text when the file does not exist or cannot be read.
    """
    try:
        base = Path(__file__).parent / "prompts"
        path = base / filename
        return path.read_text(encoding="utf-8").strip()
    except Exception:
        return default_text.strip()

IDP_COPILOT_SYSTEM_PROMPT = """You are an advanced Internal Developer Platform (IDP) Copilot Agent with production-grade capabilities.

## CORE RESPONSIBILITIES:
1. Analyze developer requests for creating development projects (microservices, libraries, applications)
2. Create detailed, executable checklists with proper dependency management
3. Execute automation tools with retry logic and error handling
4. Maintain persistent state for workflow recovery
5. Provide real-time progress updates and handle failures gracefully

## WORKFLOW PRINCIPLES:
- **Reliability First**: Always handle errors gracefully and provide recovery options
- **Transparency**: Keep users informed of progress and any issues
- **Automation**: Maximize automation while maintaining quality
- **Best Practices**: Apply industry standards and organizational guidelines
- **Idempotency**: Ensure operations can be safely retried

## PROJECT TYPE WORKFLOWS:

### Microservice:
1. Validate project requirements and naming
2. Search knowledge base for guidelines
3. Create Git repository with branch protection
4. Apply microservice template
5. Configure CI/CD pipeline
6. Set up monitoring and logging
7. Create Kubernetes manifests
8. Deploy to staging environment
9. Run integration tests
10. Generate documentation

### Library:
1. Validate library name and purpose
2. Create repository with library template
3. Set up package configuration
4. Configure testing framework
5. Set up publishing pipeline
6. Create example usage
7. Generate API documentation

### Frontend Application:
1. Validate application requirements
2. Create repository
3. Apply frontend framework template
4. Set up build system
5. Configure CDN and deployment
6. Set up E2E testing
7. Create staging deployment

## DECISION FRAMEWORK:
- Missing critical information → ASK_USER with specific questions
- No checklist exists → CREATE_CHECKLIST based on project type
- Checklist has executable items → TOOL_CALL for next item
- Tool execution failed → ERROR_RECOVERY with retry or skip
- All items complete/skipped → COMPLETE with summary

## ERROR HANDLING STRATEGY:
1. **Retry with backoff**: For transient failures (network, timeouts)
2. **Parameter adjustment**: For validation failures (invalid names, missing params)
3. **Skip and continue**: For non-critical failures with blocked dependencies
4. **Escalate to user**: For critical failures requiring manual intervention

## TOOL INTERACTION:
- Always validate tool parameters before execution
- Use appropriate timeouts based on operation type
- Collect structured feedback for continuous improvement
- Update checklist status after every tool execution

If KB Guidelines: no → use standard templates and default organizational policies.

Remember: You are a production system. Prioritize reliability, observability, and user experience."""

IDP_COPILOT_SYSTEM_PROMPT_GIT = """You are an Internal Developer Platform (IDP) Copilot Agent focused on a minimal, reliable Git workflow.

## SCOPE (ITERATION 1):
- Guide the user to create a new project repository locally with Git AND on the remote (GitHub).
- Perform tasks up to and including: local init, initial commit, remote repo creation, setting origin, and pushing 'main'.


## CORE RESPONSIBILITIES:
1. Validate or infer a valid kebab-case project name.
2. Create a local Git repository with an initial README and first commit.
3. Create a GitHub repository (requires GITHUB_TOKEN; optionally GITHUB_ORG or GITHUB_OWNER) and push 'main'.
4. Provide clear, minimal outputs and next steps.

## ALLOWED TOOLS FOR THIS ITERATION:
- validate_project_name_and_type
- create_repository

Avoid calling any other tools. If additional steps are requested, ask the user to confirm expanding scope.

## DECISION FRAMEWORK:
- On each step, check if a checklist exists; if not, create one. Keep it updated after every tool execution.
- If project name is missing or invalid → ASK_USER for a valid kebab-case name.
- If name is valid → TOOL_CALL validate_project_name_and_type → TOOL_CALL create_repository.
- After successful repository creation and remote push → COMPLETE with local path, remote URL, and commit hash.

## EXECUTION RULES:
- The repository must actually be created on disk, with a 'main' branch, README.md, and an initial commit.
- If GITHUB_TOKEN is set, create the remote repo via GitHub API (org if GITHUB_ORG is set, else under the token's user), add origin, and push 'main'. If not set, return a clear error noting local repo was created.
- If Git is not available, return a clear error and suggested installation steps.
- If the target directory already exists and is non-empty, report the conflict and stop.
- After each tool run, update the checklist state.
- Prefer clarity and reliability over breadth.

Output concise progress updates suitable for a CLI. Keep the scope strictly limited to repository creation in this iteration."""


# ===== Modular prompt exports (Phase 4) =====

ORCHESTRATOR_PROMPT = _load_prompt_file(
    "orchestrator.txt",
    """You are the Orchestrator Agent.
Maintain the master plan as structured tasks (single source of truth).
Delegate to sub-agents or call tools. Never edit Markdown via LLM; Markdown is a deterministic view.
Ask the user only when blocking information is missing. Enforce ownership and versioning for sub-agent patches.
Keep responses concise and focused on the next best action.""",
)

SCAFFOLD_PROMPT = _load_prompt_file(
    "scaffold.txt",
    """You are the Scaffolding Agent.
Your focus: repository creation, templates, and initial project structure.
Use available tools safely and return precise, verifiable results. Do not change the master plan directly; suggest patches.""",
)

DELIVERY_PROMPT = _load_prompt_file(
    "delivery.txt",
    """You are the Delivery Agent.
Your focus: CI/CD, testing, deployment, and observability setup.
Act deterministically, report actionable results, and propose patches to the master plan when needed.""",
)






// Relative Path: prototype\statemanager.py

# ==================== STATE MANAGEMENT ====================

from datetime import datetime
from pathlib import Path
import pickle
import time
from typing import Dict, Optional

import structlog


class StateManager:
    """Manages agent state persistence and recovery"""
    
    def __init__(self, state_dir: str = "./agent_states"):
        self.state_dir = Path(state_dir)
        self.state_dir.mkdir(exist_ok=True)
        self.logger = structlog.get_logger()
    
    async def save_state(self, session_id: str, state_data: Dict) -> bool:
        """Save agent state asynchronously"""
        try:
            state_file = self.state_dir / f"{session_id}.pkl"
            
            state_to_save = {
                'session_id': session_id,
                'timestamp': datetime.now().isoformat(),
                'state_data': state_data
            }
            
            # Async file write
            import aiofiles
            async with aiofiles.open(state_file, 'wb') as f:
                await f.write(pickle.dumps(state_to_save))
            
            self.logger.info("state_saved", session_id=session_id)
            return True
            
        except Exception as e:
            self.logger.error("state_save_failed", session_id=session_id, error=str(e))
            return False
    
    async def load_state(self, session_id: str) -> Optional[Dict]:
        """Load agent state asynchronously"""
        try:
            state_file = self.state_dir / f"{session_id}.pkl"
            
            if not state_file.exists():
                return None
            
            import aiofiles
            async with aiofiles.open(state_file, 'rb') as f:
                content = await f.read()
                state = pickle.loads(content)
            
            self.logger.info("state_loaded", session_id=session_id)
            return state['state_data']
            
        except Exception as e:
            self.logger.error("state_load_failed", session_id=session_id, error=str(e))
            return None
    
    def cleanup_old_states(self, days: int = 7):
        """Remove states older than specified days"""
        cutoff_time = time.time() - (days * 24 * 60 * 60)
        
        for state_file in self.state_dir.glob("*.pkl"):
            if state_file.stat().st_mtime < cutoff_time:
                state_file.unlink()
                self.logger.info("old_state_removed", file=state_file.name)




// Relative Path: prototype\todolist_actions.py
from __future__ import annotations

from typing import Any, Dict, Optional, Callable, Awaitable


# Prefer todolist helpers; aliases exist elsewhere for checklist compatibility
try:  # pragma: no cover - import shim
    from .todolist_md import create_todolist_md, update_todolist_md  # type: ignore
except Exception:  # pragma: no cover - runtime import fallback
    from todolist_md import create_todolist_md, update_todolist_md  # type: ignore


def normalize_todolist_action_name(action_name: str) -> str:
    """Normalize common LLM variants to canonical todolist action names.

    Returns the normalized action name (e.g., "create_todolist").
    """
    normalized = action_name.strip().lower().replace("-", "_").replace(" ", "_")
    if normalized in (
        "create_todolist",
        "create_checklist",
        "create_microservice_checklist",
        "create_a_microservice_checklist",
    ):
        return "create_todolist"
    return normalized


async def create_todolist(
    *,
    llm: Any,
    context: Dict[str, Any],
    system_prompt: str,
    session_id: Optional[str],
    logger: Any,
) -> str:
    """Create a Todo List using the Markdown helper and update the agent context.

    Best-effort fallback is attempted on failure; context keys remain
    backward-compatible (both todolist_* and checklist_* are maintained).
    """
    # Derive generic project info from context/user request to keep agent generic
    user_request = context.get("user_request", "") or ""
    # Simple slug from request
    try:
        lower = str(user_request).strip().lower()
        filtered = "".join(ch if (ch.isalnum() or ch in {" ", "-"}) else " " for ch in lower)
        words = [w for w in filtered.split() if w]
        project_name = "-".join(words[:6]) or "project"
    except Exception:
        project_name = "project"
    project_type = "generic"
    requirements: Dict[str, Any] = {}
    try:
        filepath = await create_todolist_md(
            llm,
            user_request=user_request,
            system_prompt=system_prompt,
            session_id=session_id,
        )
        context["todolist_created"] = True
        context["todolist_file"] = filepath
        # Back-compat keys
        context["checklist_created"] = True
        context["checklist_file"] = filepath
        # No project metadata in checklist-only mode
        return f"Created Todo List (saved to {filepath})"

    except Exception as e:
        logger.error("todolist_creation_failed", error=str(e))
        try:
            filepath = await create_todolist_md(
                llm,
                user_request=user_request,
                system_prompt=system_prompt,
                session_id=session_id,
            )
            context["todolist_created"] = True
            context["todolist_file"] = filepath
            context["checklist_created"] = True
            context["checklist_file"] = filepath
            return f"Created minimal Todo List (saved to {filepath}). Original error: {e}"
        except Exception as inner_e:
            logger.error("todolist_fallback_failed", error=str(inner_e))
            return f"Failed to create Todo List: {e}"


async def update_item_status(
    *,
    llm: Any,
    context: Dict[str, Any],
    system_prompt: str,
    session_id: Optional[str],
    parameters: Dict[str, Any],
) -> str:
    """Update a specific item in the Todo List via the Markdown helper.

    Expected parameters: item_id, status, notes (optional), result (optional).
    """
    # Checklist-only mode does not require project name; all ops are session-based

    item_id = parameters.get("item_id")
    status_text = parameters.get("status")
    notes = parameters.get("notes")
    result_txt = parameters.get("result")

    instruction_parts = [f"Set task {item_id} status to {status_text}."]
    if notes:
        instruction_parts.append(f"Add note to task {item_id}: {notes}.")
    if result_txt:
        instruction_parts.append(f"Record result for task {item_id}: {result_txt}.")
    instruction = " ".join(instruction_parts)

    filepath = await update_todolist_md(
        llm,
        instruction=instruction,
        system_prompt=system_prompt,
        session_id=session_id,
    )
    context["todolist_file"] = filepath
    # Back-compat key
    context["checklist_file"] = filepath
    return f"Updated Todo List (saved to {filepath})"


def get_next_executable_item(*, context: Dict[str, Any]) -> str:
    """Return a guidance message to open the current Todo List file."""
    path = context.get("todolist_file") or context.get("checklist_file")
    return f"Open and follow the Todo List: {path}" if path else "No Todo List created"






// Relative Path: prototype\todolist_md.py
from __future__ import annotations

from pathlib import Path
from datetime import datetime
from typing import Any, Dict, Optional
import re


# ---------- helpers ----------

def _slugify(name: str) -> str:
    allowed = "abcdefghijklmnopqrstuvwxyz0123456789-_"
    slug = (
        name.strip().lower().replace(" ", "-")
        .replace("/", "-").replace("\\", "-")
    )
    return "".join(ch for ch in slug if ch in allowed)


def get_todolist_path(session_id: Optional[str] = None, base_dir: str = "./checklists") -> Path:
    todolist_dir = Path(base_dir)
    todolist_dir.mkdir(parents=True, exist_ok=True)
    sid = session_id or datetime.now().strftime("%Y%m%d%H%M%S")
    # Sanitize for filesystem safety (Windows: : * ? " < > | and slashes)
    if sid:
        sid = re.sub(r'[^A-Za-z0-9._-]', '_', str(sid))[:120]
    # keep one stable file per session id, otherwise always use a single shared file
    name = f"todolist_{sid}.md" if session_id else "todolist.md"
    return todolist_dir / name


def _ensure_file(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    if not path.exists():
        path.write_text("", encoding="utf-8")


# ---------- public API (wrappers the agent expects) ----------

async def create_todolist_md(
    llm: Any,
    *,
    user_request: str,
    system_prompt: str,
    session_id: Optional[str] = None,
    base_dir: str = "./checklists",
) -> str:
    """
    Erzeugt eine vollständige Todo-Liste (Markdown) mit den Sektionen:
    - Title
    - Meta (created/last-updated)
    - Tasks (Checkbox-Liste; JEDES ausführbare Item enthält im Text `tool: <exact_tool_name>`)
    - Open Questions (awaiting user)
    - Notes

    Rückgabe: Dateipfad als String.
    """
    guide = """
Create a concise, executable TODO plan in pure Markdown.

Sections (in this exact order):
1) # Todo List
2) ## Meta
   - created: <ISO8601>
   - last-updated: <ISO8601>
3) ## Tasks
   - [ ] <short task> (tool: <exact_tool_name>)  // each executable task includes 'tool: <...>'
   - Keep tasks atomic, verifiable, and implementation-oriented.
   - Prefer 3–10 tasks.
4) ## Open Questions (awaiting user)
   - Bullet list of clarifying questions that are nice-to-have (non-blocking).
   - If none, keep the section but leave it empty.
5) ## Notes
   - Free-form notes/results appended as the workflow progresses.
   - If none, keep the section but leave it empty.

Strict rules:
- Output ONLY Markdown (no code fences, no extra commentary).
- Keep wording short and unambiguous.
- DO NOT remove the section headings.
- If you are unsure about tools, still propose tasks; the agent will reconcile tools later.
    """.strip()

    prompt = f"{guide}\n\nContext/user request:\n{user_request}"
    md = await llm.generate_response(prompt, system_prompt=system_prompt)

    path = get_todolist_path(session_id=session_id, base_dir=base_dir)
    _ensure_file(path)
    path.write_text(md, encoding="utf-8")
    return str(path)


async def update_todolist_md(
    llm: Any,
    *,
    instruction: str,
    system_prompt: str,
    session_id: Optional[str] = None,
    base_dir: str = "./checklists",
) -> str:
    """
    Aktualisiert eine bestehende Todo-Liste anhand einer Anweisung.
    Beispiele für 'instruction':
      - "Find the task that corresponds to tool 'create_repository' and mark it IN_PROGRESS."
      - "Under 'Open Questions (awaiting user)', add these bullets: ... "
      - "Set task 3 status to COMPLETED. Record result: {...}"

    Rückgabe: Dateipfad als String.
    """
    path = get_todolist_path(session_id=session_id, base_dir=base_dir)
    _ensure_file(path)
    current_md = path.read_text(encoding="utf-8")

    # Der Editor-Prompt hält die Struktur stabil und lässt Anweisungen gezielt anwenden.
    prompt = f"""
You are a precise editor of a structured Markdown TODO document.

Document shape MUST remain:
# Todo List
## Meta
- created: <ISO8601>
- last-updated: <ISO8601>
## Tasks
- [ ] ...
## Open Questions (awaiting user)
- ...
## Notes
- ...

Apply the instruction to the document.

Rules:
- Return ONLY the full updated Markdown (no comments, no code fences).
- Preserve existing sections and ordering.
- If a section is missing, recreate it with the correct heading.
- When modifying Tasks:
  - Keep checkbox format "- [ ]" or "- [x]" (we accept text status like IN_PROGRESS/COMPLETED as suffixes if requested),
  - Prefer to identify tasks by 'tool: <name>' or by stable numeric prefix if present.
- Update the 'last-updated' field in Meta to current ISO8601 (you can infer a plausible timestamp).
- Do not change 'created' value.

Instruction:
{instruction}

Current Document:
{current_md}
""".strip()

    updated_md = await llm.generate_response(prompt, system_prompt=system_prompt)

    path.write_text(updated_md, encoding="utf-8")
    return str(path)


# ---------- deterministic rendering from structured state ----------
def render_todolist_markdown(
    *,
    tasks: list[dict],
    open_questions: list[str] | None,
    session_id: Optional[str] = None,
    base_dir: str = "./checklists",
) -> str:
    """Render the Markdown Todo List from in-memory structured state.

    This function is the single renderer: it does not call the LLM and ensures
    the Markdown view is always consistent with the authoritative task state.

    Args:
        tasks: List of task dictionaries containing at least id, title, status, optional tool/params/notes.
        open_questions: Optional list of clarifying questions.
        session_id: Current session identifier used to determine the file path.
        base_dir: Directory to store Markdown files.

    Returns:
        The file path where the Markdown view was written.
    """
    # Defensive: allow callers to suppress rendering entirely (e.g., sub-agents)
    if isinstance(tasks, dict) and tasks.get("suppress_markdown"):
        return ""

    path = get_todolist_path(session_id=session_id, base_dir=base_dir)
    _ensure_file(path)

    created = datetime.now().isoformat()
    last_updated = datetime.now().isoformat()

    lines: list[str] = []
    lines.append("# Todo List")
    lines.append("## Meta")
    lines.append(f"- created: {created}")
    lines.append(f"- last-updated: {last_updated}")
    lines.append("## Tasks")

    def _checkbox(status: str) -> str:
        # Render checkbox based on status; text status retained for clarity
        checked = status.upper() in {"COMPLETED"}
        box = "[x]" if checked else "[ ]"
        return box

    for t in tasks:
        tid = str(t.get("id", ""))
        title = str(t.get("title", "")).strip() or tid
        status = str(t.get("status", "PENDING")).upper()
        tool = t.get("tool")
        executor_id = t.get("executor_id")
        action = t.get("action")
        params = t.get("params") or {}
        notes = str(t.get("notes", "")).strip()
        owner = t.get("owner_agent")

        suffix_parts: list[str] = [f"status: {status}"]
        if tool:
            suffix_parts.append(f"tool: {tool}")
        if executor_id:
            suffix_parts.append(f"executor: {executor_id}")
        if action:
            suffix_parts.append(f"action: {action}")
        if params:
            # keep params compact to avoid noisy Markdown
            suffix_parts.append(f"params: {params}")
        if owner:
            suffix_parts.append(f"owner: {owner}")
        if notes:
            suffix_parts.append(f"notes: {notes}")
        suffix = " (" + ", ".join(suffix_parts) + ")" if suffix_parts else ""

        lines.append(f"- {_checkbox(status)} {title}{suffix}")

    lines.append("## Open Questions (awaiting user)")
    oq = open_questions or []
    if oq:
        for q in oq:
            lines.append(f"- {q}")
    lines.append("## Notes")

    md = "\n".join(lines) + "\n"
    path.write_text(md, encoding="utf-8")
    return str(path)


# ---------- backwards-compat (1 release grace) ----------

create_checklist_md = create_todolist_md  # type: ignore
update_checklist_md = update_todolist_md  # type: ignore
get_checklist_path = get_todolist_path    # type: ignore




// Relative Path: prototype\tools.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Awaitable, Union
import asyncio
import concurrent.futures
import json


JsonDict = Dict[str, Any]


@dataclass
class ToolSpec:
    name: str
    description: str
    input_schema: Dict[str, Any]
    output_schema: Dict[str, Any]
    func: Callable[..., Union[JsonDict, Awaitable[JsonDict]]]
    is_async: bool = True
    timeout: Optional[float] = None
    aliases: Optional[List[str]] = None
    # Optional provider that returns an ExecutorCapabilities-like dict describing actions
    # This enables capability handshakes for sub-agents and tools.
    capabilities_provider: Optional[Callable[[], Dict[str, Any]]] = None


def _normalize(name: str) -> str:
    return name.strip().lower().replace("-", "_").replace(" ", "_")


def export_openai_tools(tools: List[ToolSpec]) -> List[Dict[str, Any]]:
    return [
        {
            "type": "function",
            "function": {
                "name": t.name,
                "description": t.description,
                "parameters": t.input_schema,
            },
        }
        for t in tools
    ]


def find_tool(tools: List[ToolSpec], name_or_alias: str) -> Optional[ToolSpec]:
    if not name_or_alias:
        return None
    target = _normalize(name_or_alias)
    for t in tools:
        if _normalize(t.name) == target:
            return t
        if t.aliases:
            for a in t.aliases:
                if _normalize(a) == target:
                    return t
    return None


async def execute_tool(tool: ToolSpec, params: Dict[str, Any]) -> Dict[str, Any]:
    try:
        if tool.is_async or asyncio.iscoroutinefunction(tool.func):
            coro = tool.func(**params)  # type: ignore[misc]
            if not asyncio.iscoroutine(coro):
                # In case is_async was set but func isn't actually async
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = loop.run_in_executor(executor, lambda: tool.func(**params))  # type: ignore[misc]
                    if tool.timeout:
                        result = await asyncio.wait_for(future, timeout=tool.timeout)
                    else:
                        result = await future
            else:
                if tool.timeout:
                    result = await asyncio.wait_for(coro, timeout=tool.timeout)
                else:
                    result = await coro
        else:
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = loop.run_in_executor(executor, lambda: tool.func(**params))  # type: ignore[misc]
                if tool.timeout:
                    result = await asyncio.wait_for(future, timeout=tool.timeout)
                else:
                    result = await future

        if isinstance(result, dict):
            return result
        return {"success": True, "result": result}
    except asyncio.TimeoutError:
        return {"success": False, "error": f"Tool '{tool.name}' timed out"}
    except Exception as e:
        return {"success": False, "error": str(e)}


async def execute_tool_by_name(tools: List[ToolSpec], name_or_alias: str, params: Dict[str, Any]) -> Dict[str, Any]:
    spec = find_tool(tools, name_or_alias)
    if not spec:
        return {"success": False, "error": f"Tool '{name_or_alias}' not found"}
    return await execute_tool(spec, params)


# === Lookup index helpers ===
def build_tool_index(tools: List[ToolSpec]) -> Dict[str, ToolSpec]:
    """Create a lookup index for tools and their aliases using normalized keys.

    Keys normalize hyphens/whitespace and case to ensure consistent resolution.
    """
    index: Dict[str, ToolSpec] = {}
    for tool in tools:
        index[_normalize(tool.name)] = tool
        if tool.aliases:
            for alias in tool.aliases:
                index[_normalize(alias)] = tool
    return index


async def execute_tool_by_name_from_index(index: Dict[str, ToolSpec], name_or_alias: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """Execute a tool resolved via a prebuilt index. Returns a standard result dict."""
    if not name_or_alias:
        return {"success": False, "error": "Tool name is empty"}
    spec = index.get(_normalize(name_or_alias))
    if not spec:
        return {"success": False, "error": f"Tool '{name_or_alias}' not found"}
    # Best-effort param sanitization: ensure JSON-serializable
    try:
        json.dumps(params)
    except Exception:
        params = {k: str(v) for k, v in (params or {}).items()}
    return await execute_tool(spec, params)


def merge_tool_specs(*tool_groups: List[ToolSpec]) -> List[ToolSpec]:
    """Merge multiple tool lists by normalized name, last one wins.

    This allows composing BUILTIN_TOOLS with agent-tools without duplicates.
    """
    merged: Dict[str, ToolSpec] = {}
    for group in tool_groups:
        for t in group or []:
            merged[_normalize(t.name)] = t
    return list(merged.values())





// Relative Path: prototype\tools_builtin.py
from __future__ import annotations
import asyncio
import re
from pathlib import Path
import os
import json
import shutil
from datetime import datetime
import asyncio.subprocess as asp
from typing import Any, Dict, List

from .tools import ToolSpec
from .agent import ReActAgent
from .llm_provider import LLMProvider
from .tools import build_tool_index, execute_tool_by_name_from_index


# ==== Tool Implementations (migrated from idp.py methods) ====


async def create_repository(name: str, visibility: str = "private", **kwargs) -> Dict[str, Any]:
    """Create a real local Git repository with an initial commit.

    Steps:
    - Create directory ./<name>
    - git init, set branch to 'main'
    - write README.md and commit
    - configure local user.name/email if missing
    """
    if not isinstance(name, str) or not name.strip():
        return {"success": False, "error": "Missing or invalid repository name"}
    if any(char in name for char in "@!#$%^&*()"):
        return {"success": False, "error": "Invalid repository name"}

    if shutil.which("git") is None:
        return {
            "success": False,
            "error": "Git not found in PATH. Please install Git and retry.",
        }

    repo_dir = Path.cwd() / name
    if repo_dir.exists():
        if any(repo_dir.iterdir()):
            return {"success": False, "error": f"Target directory '{repo_dir}' already exists and is not empty"}
    else:
        try:
            repo_dir.mkdir(parents=True, exist_ok=False)
        except Exception as e:
            return {"success": False, "error": f"Failed to create directory: {e}"}

    async def run_git(args: List[str]) -> Dict[str, Any]:
        proc = await asp.create_subprocess_exec(
            "git", *args, cwd=str(repo_dir), stdout=asp.PIPE, stderr=asp.PIPE
        )
        out_b, err_b = await proc.communicate()
        return {"code": proc.returncode, "stdout": out_b.decode().strip(), "stderr": err_b.decode().strip()}

    init_res = await run_git(["init"])
    if init_res["code"] != 0:
        return {"success": False, "error": f"git init failed: {init_res['stderr']}"}

    branch_res = await run_git(["branch", "-M", "main"])
    if branch_res["code"] != 0:
        return {"success": False, "error": f"setting branch failed: {branch_res['stderr']}"}

    cfg_email = await run_git(["config", "--get", "user.email"])
    if cfg_email["code"] != 0 or not cfg_email["stdout"]:
        set_email = await run_git(["config", "user.email", "idp@example.com"])
        if set_email["code"] != 0:
            return {"success": False, "error": f"git config user.email failed: {set_email['stderr']}"}
    cfg_name = await run_git(["config", "--get", "user.name"])
    if cfg_name["code"] != 0 or not cfg_name["stdout"]:
        set_name = await run_git(["config", "user.name", "IDP Copilot"])
        if set_name["code"] != 0:
            return {"success": False, "error": f"git config user.name failed: {set_name['stderr']}"}

    try:
        readme = repo_dir / "README.md"
        if not readme.exists():
            readme.write_text(
                f"# {name}\n\nCreated by IDP Copilot on {datetime.now().isoformat()}\n",
                encoding="utf-8",
            )
    except Exception as e:
        return {"success": False, "error": f"Failed to write README.md: {e}"}

    add_res = await run_git(["add", "README.md"])
    if add_res["code"] != 0:
        return {"success": False, "error": f"git add failed: {add_res['stderr']}"}
    commit_res = await run_git(["commit", "-m", "Initial commit"])
    if commit_res["code"] != 0:
        return {"success": False, "error": f"git commit failed: {commit_res['stderr']}"}
    rev_res = await run_git(["rev-parse", "HEAD"])
    if rev_res["code"] != 0:
        return {"success": False, "error": f"git rev-parse failed: {rev_res['stderr']}"}

    # Create remote on GitHub and push initial commit if token is available
    token = os.getenv("GITHUB_TOKEN")
    org = os.getenv("GITHUB_ORG")
    owner_env = os.getenv("GITHUB_OWNER")
    if not token:
        return {
            "success": False,
            "error": "GITHUB_TOKEN is not set. Cannot create remote repository.",
            "local_repo_created": True,
            "repo_path": str(repo_dir),
            "default_branch": "main",
            "initial_commit": rev_res["stdout"],
        }

    import urllib.request
    import urllib.error

    api_url = f"https://api.github.com/user/repos"
    if org:
        api_url = f"https://api.github.com/orgs/{org}/repos"
    payload = {
        "name": name,
        "private": (str(visibility).lower() != "public"),
        "auto_init": False,
        "has_issues": True,
        "has_projects": True,
        "has_wiki": False,
        "default_branch": "main",
    }
    req = urllib.request.Request(
        api_url,
        data=json.dumps(payload).encode("utf-8"),
        headers={
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "User-Agent": "idp-copilot",
            "Content-Type": "application/json",
        },
        method="POST",
    )
    try:
        with urllib.request.urlopen(req) as resp:
            body = resp.read().decode("utf-8")
            repo_info = json.loads(body)
    except urllib.error.HTTPError as e:
        try:
            err_body = e.read().decode("utf-8")
            err_json = json.loads(err_body)
            msg = err_json.get("message", err_body)
        except Exception:
            msg = str(e)
        return {
            "success": False,
            "error": f"GitHub repo creation failed: {msg}",
            "local_repo_created": True,
            "repo_path": str(repo_dir),
            "default_branch": "main",
            "initial_commit": rev_res["stdout"],
        }

    owner_login = ((repo_info or {}).get("owner") or {}).get("login") or owner_env or org or ""
    if not owner_login:
        return {
            "success": False,
            "error": "Could not determine repository owner for remote URL.",
            "local_repo_created": True,
            "repo_path": str(repo_dir),
            "default_branch": "main",
            "initial_commit": rev_res["stdout"],
        }

    token_remote_url = f"https://x-access-token:{token}@github.com/{owner_login}/{name}.git"
    clean_remote_url = f"https://github.com/{owner_login}/{name}.git"

    add_remote = await run_git(["remote", "add", "origin", token_remote_url])
    if add_remote["code"] != 0:
        return {"success": False, "error": f"git remote add failed: {add_remote['stderr']}", "local_repo_created": True, "repo_path": str(repo_dir)}
    push_res = await run_git(["push", "-u", "origin", "main"])
    if push_res["code"] != 0:
        return {"success": False, "error": f"git push failed: {push_res['stderr']}", "local_repo_created": True, "repo_path": str(repo_dir)}
    set_url = await run_git(["remote", "set-url", "origin", clean_remote_url])
    if set_url["code"] != 0:
        return {"success": False, "error": f"git remote set-url failed: {set_url['stderr']}", "local_repo_created": True, "repo_path": str(repo_dir)}

    return {
        "success": True,
        "repo_path": str(repo_dir),
        "default_branch": "main",
        "initial_commit": rev_res["stdout"],
        "remote_html_url": repo_info.get("html_url"),
        "remote_clone_url": clean_remote_url,
    }


async def setup_branch_protection(repo_name: str, **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(1)
    return {
        "success": True,
        "rules": [
            "require-pr-reviews",
            "dismiss-stale-reviews",
            "require-status-checks",
        ],
    }


async def create_git_repository_with_branch_protection(
    repo_name: str = None, visibility: str = "private", **kwargs
) -> Dict[str, Any]:
    visibility = kwargs.get("visibility", visibility)
    name = repo_name or kwargs.get("name") or kwargs.get("project_name") or "unnamed"
    create = await create_repository(name=name, visibility=visibility, **kwargs)
    if not create.get("success"):
        return create
    protection = await setup_branch_protection(repo_name=name, **kwargs)
    if not protection.get("success"):
        return protection
    return {"success": True, "repo": create, "branch_protection": protection}


async def list_templates(project_type: str = None, **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(0.5)
    templates = {
        "microservice": ["fastapi-microservice", "spring-boot-service", "go-microservice"],
        "library": ["python-library", "typescript-library", "java-library"],
        "frontend": ["nextjs-app", "react-spa", "vue-app"],
    }
    if project_type and project_type in templates:
        return {"success": True, "templates": {project_type: templates[project_type]}}
    return {"success": True, "templates": templates}


async def apply_template(template: str, target_path: str, **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(3)
    return {
        "success": True,
        "files_created": [
            "src/main.py",
            "tests/test_main.py",
            "README.md",
            "Dockerfile",
        ],
        "next_steps": ["Configure environment variables", "Update README"],
    }


async def setup_cicd(repo_path: str, pipeline_type: str = "github-actions", **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(2)
    return {
        "success": True,
        "pipeline_file": f".github/workflows/ci.yml",
        "stages": ["lint", "test", "build", "security-scan"],
    }


async def setup_observability(project_name: str = None, **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(1)
    return {
        "success": True,
        "stack": ["prometheus", "grafana", "otel"],
        "notes": "Integrated default dashboards and traces",
    }


async def run_tests(project_path: str, **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(5)
    return {"success": True, "tests_run": 42, "tests_passed": 42, "coverage": "87%"}


async def generate_k8s_manifests(service_name: str = None, **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(1)
    name = service_name or kwargs.get("project_name") or "service"
    return {
        "success": True,
        "files": [
            f"k8s/{name}-deployment.yaml",
            f"k8s/{name}-service.yaml",
            f"k8s/{name}-configmap.yaml",
        ],
    }


async def create_k8s_namespace(name: str, **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(1)
    return {
        "success": True,
        "namespace": name,
        "resources": ["namespace", "resource-quota", "network-policy"],
    }


async def deploy_to_staging(project: str, version: str = "latest", **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(8)
    return {
        "success": True,
        "environment": "staging",
        "url": f"https://{project}-staging.company.io",
        "version": version,
    }


async def generate_documentation(project_name: str = None, **kwargs) -> Dict[str, Any]:
    await asyncio.sleep(1)
    name = project_name or "project"
    return {
        "success": True,
        "artifacts": [f"docs/{name}-api.md", f"docs/{name}-operations.md"],
    }


async def validate_project_name_and_type(
    project_name: str = None, project_type: str = None, programming_language: str = None, **kwargs
) -> Dict[str, Any]:
    await asyncio.sleep(0)
    if not project_name or not isinstance(project_name, str):
        return {"success": False, "error": "Missing required parameter: project_name"}
    name_pattern = re.compile(r"^[a-z0-9]+(?:-[a-z0-9]+)*$")
    if not name_pattern.match(project_name):
        return {
            "success": False,
            "error": "Invalid project name. Use kebab-case: lowercase letters, numbers, and single dashes.",
        }
    allowed_types = {"microservice", "library", "application", "frontend", "backend", "generic"}
    if project_type and project_type not in allowed_types:
        return {
            "success": False,
            "error": f"Unsupported project_type '{project_type}'. Allowed: {sorted(allowed_types)}",
        }
    details = {
        "project_name": project_name,
        "project_type": project_type or "microservice",
        "programming_language": programming_language or "python",
        "policy_checks": ["kebab-case", "allowed_type"],
    }
    return {"success": True, "result": details}


async def search_knowledge_base_for_guidelines(
    project_type: str = None, language: str = None, project_name: str = None, **kwargs
) -> Dict[str, Any]:
    try:
        repo_root = Path.cwd()
        base_paths = [
            repo_root / "capstone" / "backend" / "documents" / "guidelines",
            repo_root / "capstone" / "documents" / "guidelines",
            repo_root / "capstone" / "backend" / "documents",
        ]
        keywords: List[str] = []
        if project_type:
            keywords.append(str(project_type).lower())
        if language:
            keywords.append(str(language).lower())
        if project_name:
            keywords.append(str(project_name).lower())
        keywords.extend(["service", "microservice", "standards", "guidelines", "ci/cd", "cicd"])
        matched: List[Dict[str, Any]] = []
        scanned_files: List[str] = []
        for base in base_paths:
            if not base.exists() or not base.is_dir():
                continue
            for file in base.glob("**/*.md"):
                scanned_files.append(str(file))
                try:
                    text = file.read_text(encoding="utf-8", errors="ignore")
                except Exception:
                    continue
                text_lower = text.lower()
                filename_lower = file.name.lower()
                score = 0
                for kw in keywords:
                    if kw and (kw in text_lower or kw in filename_lower):
                        score += 1
                title = None
                for line in text.splitlines():
                    if line.strip().startswith("#"):
                        title = line.strip().lstrip("# ")
                        break
                if score > 0 or (not keywords and title):
                    snippets: List[str] = []
                    if keywords:
                        for line in text.splitlines():
                            line_l = line.lower()
                            if any(kw in line_l for kw in keywords) and line.strip():
                                snippets.append(line.strip())
                                if len(snippets) >= 3:
                                    break
                    matched.append(
                        {
                            "file": str(file),
                            "title": title or file.name,
                            "score": score,
                            "snippets": snippets,
                        }
                    )
        matched.sort(key=lambda m: (-m.get("score", 0), m.get("title") or ""))
        if not matched:
            defaults = []
            for default_name in [
                "python-service-standards.md",
                "cicd-pipeline-standards.md",
                "go-service-standards.md",
            ]:
                for base in base_paths:
                    candidate = base / default_name
                    if candidate.exists():
                        defaults.append(
                            {
                                "file": str(candidate),
                                "title": default_name.replace("-", " ").replace(".md", "").title(),
                                "score": 0,
                                "snippets": [],
                            }
                        )
            matched = defaults
        return {"success": True, "result": {"searched_files": scanned_files, "matches": matched[:10]}}
    except Exception as e:
        return {"success": False, "error": str(e)}


# ==== Tool Specs ====


BUILTIN_TOOLS: List[ToolSpec] = [
    ToolSpec(
        name="create_repository",
        description="Creates local Git repo and GitHub remote, pushes initial commit",
        input_schema={
            "type": "object",
            "properties": {"name": {"type": "string"}, "visibility": {"type": "string"}},
            "required": ["name"],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=create_repository,
        is_async=True,
        timeout=10,
        aliases=[],
    ),
    ToolSpec(
        name="validate_project_name_and_type",
        description="Validate project name and type",
        input_schema={
            "type": "object",
            "properties": {
                "project_name": {"type": "string"},
                "project_type": {"type": "string"},
                "programming_language": {"type": "string"},
            },
            "required": ["project_name"],
            "additionalProperties": False,
        },
        output_schema={"type": "object"},
        func=validate_project_name_and_type,
        is_async=True,
        timeout=5,
        aliases=["project-validator"],
    ),    
    ToolSpec(
        name="setup_branch_protection",
        description="Setup branch rules",
        input_schema={
            "type": "object",
            "properties": {"repo_name": {"type": "string"}},
            "required": ["repo_name"],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=setup_branch_protection,
        is_async=True,
        timeout=5,
        aliases=[],
    ),
    ToolSpec(
        name="create_git_repository_with_branch_protection",
        description="Create repo then apply standard branch protection",
        input_schema={
            "type": "object",
            "properties": {"repo_name": {"type": "string"}, "visibility": {"type": "string"}},
            "required": [],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=create_git_repository_with_branch_protection,
        is_async=True,
        timeout=20,
        aliases=["git-repo-creator", "create-git-repo"],
    ),
    
    ToolSpec(
        name="list_templates",
        description="List available templates",
        input_schema={
            "type": "object",
            "properties": {"project_type": {"type": "string"}},
            "required": [],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=list_templates,
        is_async=True,
        timeout=5,
        aliases=[],
    ),
    ToolSpec(
        name="apply_template",
        description="Apply project template",
        input_schema={
            "type": "object",
            "properties": {"template": {"type": "string"}, "target_path": {"type": "string"}},
            "required": ["template", "target_path"],
            "additionalProperties": False,
        },
        output_schema={"type": "object"},
        func=apply_template,
        is_async=True,
        timeout=15,
        aliases=["template-applier"],
    ),
    ToolSpec(
        name="setup_cicd_pipeline",
        description="Setup CI/CD pipeline",
        input_schema={
            "type": "object",
            "properties": {"repo_path": {"type": "string"}, "pipeline_type": {"type": "string"}},
            "required": ["repo_path"],
            "additionalProperties": False,
        },
        output_schema={"type": "object"},
        func=setup_cicd,
        is_async=True,
        timeout=20,
        aliases=["ci-cd-configurator"],
    ),
    ToolSpec(
        name="run_initial_tests",
        description="Run initial test suite",
        input_schema={
            "type": "object",
            "properties": {"project_path": {"type": "string"}},
            "required": ["project_path"],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=run_tests,
        is_async=True,
        timeout=30,
        aliases=["test-runner"],
    ),
    ToolSpec(
        name="create_k8s_namespace",
        description="Create K8s namespace",
        input_schema={
            "type": "object",
            "properties": {"name": {"type": "string"}},
            "required": ["name"],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=create_k8s_namespace,
        is_async=True,
        timeout=10,
        aliases=[],
    ),
    ToolSpec(
        name="deploy_to_staging",
        description="Deploy to staging",
        input_schema={
            "type": "object",
            "properties": {"project": {"type": "string"}, "version": {"type": "string"}},
            "required": ["project"],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=deploy_to_staging,
        is_async=True,
        timeout=45,
        aliases=["k8s-deployer"],
    ),
    ToolSpec(
        name="search_knowledge_base_for_guidelines",
        description="Searches local knowledge base for guidelines relevant to the project",
        input_schema={
            "type": "object",
            "properties": {
                "project_type": {"type": "string"},
                "language": {"type": "string"},
                "project_name": {"type": "string"},
            },
            "required": [],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=search_knowledge_base_for_guidelines,
        is_async=True,
        timeout=10,
        aliases=["search_knowledge_base", "kb-search", "search-guidelines"],
    ),
    ToolSpec(
        name="setup_observability",
        description="Setup monitoring & logging",
        input_schema={
            "type": "object",
            "properties": {"project_name": {"type": "string"}},
            "required": [],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=setup_observability,
        is_async=True,
        timeout=10,
        aliases=["observability-integrator"],
    ),
    ToolSpec(
        name="generate_k8s_manifests",
        description="Generate K8s manifests",
        input_schema={
            "type": "object",
            "properties": {"service_name": {"type": "string"}},
            "required": [],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=generate_k8s_manifests,
        is_async=True,
        timeout=10,
        aliases=["k8s-manifest-generator"],
    ),
    ToolSpec(
        name="generate_documentation",
        description="Generate documentation",
        input_schema={
            "type": "object",
            "properties": {"project_name": {"type": "string"}},
            "required": [],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=generate_documentation,
        is_async=True,
        timeout=10,
        aliases=["doc-generator"],
    ),
]



BUILTIN_TOOLS_SIMPLIFIED: List[ToolSpec] = [
    ToolSpec(
        name="create_repository",
        description="Creates local Git repo and GitHub remote, pushes initial commit",
        input_schema={
            "type": "object",
            "properties": {"name": {"type": "string"}, "visibility": {"type": "string"}},
            "required": ["name"],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=create_repository,
        is_async=True,
        timeout=10,
        aliases=[],
    ),
    ToolSpec(
        name="validate_project_name_and_type",
        description="Validate project name and type",
        input_schema={
            "type": "object",
            "properties": {
                "project_name": {"type": "string"},
                "project_type": {"type": "string"},
                "programming_language": {"type": "string"},
            },
            "required": ["project_name"],
            "additionalProperties": False,
        },
        output_schema={"type": "object"},
        func=validate_project_name_and_type,
        is_async=True,
        timeout=5,
        aliases=["project-validator"],
    )
]

# Expose combined tool list for agent construction where needed
ALL_TOOLS: List[ToolSpec] = BUILTIN_TOOLS


# ===== Sub-Agent Wrapper(s) as Tools =====
async def run_sub_agent(
    *,
    task: str,
    inputs: Dict[str, Any] | None = None,
    shared_context: Dict[str, Any] | None = None,
    allowed_tools: List[str] | None = None,
    budget: Dict[str, Any] | None = None,
    resume_token: str | None = None,
    answers: Dict[str, Any] | None = None,
    agent_name: str | None = None,
    **kwargs: Any,
) -> Dict[str, Any]:
    """Run a constrained sub-agent and return either a patch or need_user_input.

    This wrapper expects the hosting orchestrator to pass in the orchestrator's LLMProvider via kwargs['llm']
    and the base system prompt via kwargs['system_prompt'] for consistency.
    """
    llm: LLMProvider | None = kwargs.get("llm")
    system_prompt: str = kwargs.get("system_prompt") or ""
    if llm is None:
        return {"success": False, "error": "Missing llm provider for sub-agent"}

    # Construct tool whitelist index from BUILTIN_TOOLS
    allow = [t for t in BUILTIN_TOOLS if (not allowed_tools) or (t.name in allowed_tools)]
    subagent = ReActAgent(system_prompt=None, llm=llm, tools=allow, max_steps=int((budget or {}).get("max_steps", 12)), mission=system_prompt)

    # Seed minimal context (ephemeral + child-session) to avoid state collision
    parent_sid = (shared_context or {}).get("session_id") or "no-session"
    subagent.session_id = f"{parent_sid}:sub:{(agent_name or 'subagent')}"
    subagent.context = {
        "user_request": task,
        "known_answers_text": (shared_context or {}).get("known_answers_text", ""),
        "facts": (shared_context or {}).get("facts", {}),
        "version": int((shared_context or {}).get("version", 1)),
        "suppress_markdown": True,
        "ephemeral_state": True,
        # tag for logging and ownership
        "agent_name": agent_name or "subagent",
    }

    # Run a short loop
    transcript: List[str] = []
    async for chunk in subagent.process_request(task, session_id=subagent.session_id):
        transcript.append(chunk)

    # Inspect sub-agent state
    if subagent.context.get("awaiting_user_input"):
        return {
            "success": False,
            "need_user_input": subagent.context.get("awaiting_user_input"),
            "state_token": "opaque",  # kept simple for v1
        }

    # Build a minimal patch reflecting only status updates against master tasks
    patch = {
        "base_version": int((shared_context or {}).get("version", 1)),
        "agent_name": agent_name or "subagent",
        "ops": []
    }
    master_tasks = list((shared_context or {}).get("tasks", []))
    target_task_id = str((shared_context or {}).get("target_task_id") or "").strip() or None
    wrapper_norm = (agent_name or "subagent").strip().lower().replace("-", "_").replace(" ", "_")
    def _norm(s: str) -> str:
        return (s or "").strip().lower().replace("-", "_").replace(" ", "_")
    def _find_master_task_id_by_tool(tool_name: str) -> str | None:
        # 0) Deterministic: prefer explicitly given target_task_id
        if target_task_id:
            return target_task_id
        norm = _norm(tool_name)
        # 1) Direct tool match
        for mt in master_tasks:
            tt = _norm(mt.get("tool"))
            if tt and tt == norm:
                return str(mt.get("id"))
        # 2) Prefixed tool match: wrapper.action
        for mt in master_tasks:
            tt = _norm(mt.get("tool"))
            if tt and tt == f"{wrapper_norm}.{norm}":
                return str(mt.get("id"))
        # 3) Fallback: match by executor_id + action
        for mt in master_tasks:
            exec_id = _norm(mt.get("executor_id"))
            action = _norm(mt.get("action"))
            if action == norm and (not exec_id or exec_id == wrapper_norm):
                return str(mt.get("id"))
        return None
    for t in subagent.context.get("tasks", []):
        status = str(t.get("status","")).upper()
        tool_name = t.get("tool")
        if tool_name and status in {"IN_PROGRESS","COMPLETED"}:
            tid = _find_master_task_id_by_tool(tool_name)
            if tid:
                patch["ops"].append({"op":"update","task_id":tid,"fields":{"status":status}})

    return {"success": True, "patch": patch, "result": {"transcript": "".join(transcript)}}


# Example sub-agent ToolSpec (scaffolder)
AGENT_TOOLS: List[ToolSpec] = [
    ToolSpec(
        name="agent_scaffold_webservice",
        description="Sub-agent: scaffolds a webservice using whitelisted tools",
        input_schema={
            "type": "object",
            "properties": {
                "task": {"type": "string"},
                "inputs": {"type": "object"},
                "shared_context": {"type": "object"},
                "allowed_tools": {"type": "array", "items": {"type": "string"}},
                "budget": {"type": "object"},
                "resume_token": {"type": "string"},
                "answers": {"type": "object"},
            },
            "required": ["task"],
            "additionalProperties": True,
        },
        output_schema={"type": "object"},
        func=run_sub_agent,
        is_async=True,
        timeout=120,
        aliases=["agent_scaffold", "agent_webservice"],
    )
]

ALL_TOOLS_WITH_AGENTS: List[ToolSpec] = BUILTIN_TOOLS + AGENT_TOOLS



// Relative Path: prototype\__init__.py
"""Prototype package exports.

Exposes a unified ALL_TOOLS for convenience when composing agents.
"""

from .tools_builtin import BUILTIN_TOOLS, BUILTIN_TOOLS_SIMPLIFIED  # noqa: F401

__all__ = [
    "BUILTIN_TOOLS",
    "BUILTIN_TOOLS_SIMPLIFIED",
]





// Relative Path: tests\conftest.py
from __future__ import annotations

"""Pytest configuration to ensure imports work when running from the capstone folder.

This adds the repository root to sys.path so that imports like
`from capstone.backend.app.main import app` resolve correctly.
"""

import sys
from pathlib import Path


def _ensure_repo_root_on_syspath() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    path_str = str(repo_root)
    if path_str not in sys.path:
        sys.path.insert(0, path_str)


_ensure_repo_root_on_syspath()






// Relative Path: tests\test_api.py
from __future__ import annotations

"""Integration tests covering the public API endpoints.

These tests use the real OpenAI provider. Ensure OPENAI_API_KEY is set.
"""

import os
from typing import Dict

import httpx
import pytest


def _require_openai() -> None:
    """Assert that OPENAI_API_KEY is available for real-LLM tests."""
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    assert api_key, "OPENAI_API_KEY must be set to run integration tests with real LLM"


async def _register_minimal_system(client: httpx.AsyncClient) -> str:
    """Register a minimal orchestrator-only AgentSystem and return its id."""
    doc: Dict[str, object] = {
        "version": 1,
        "system": {"name": "api-suite"},
        "agents": [
            {
                "id": "orchestrator",
                "role": "orchestrator",
                "system_prompt": "You are the orchestrator.",
                "mission": "Create a short plan and validate inputs.",
                "max_steps": 3,
                "tools": {"allow": ["validate_project_name_and_type"]},
                "model": {"provider": "openai", "model": "gpt-4.1", "temperature": 0.1},
            }
        ],
    }
    res = await client.post("/agent-systems", json=doc)
    assert res.status_code == 201, res.text
    return str(res.json()["id"])


@pytest.mark.asyncio
async def test_tools_and_agent_systems() -> None:
    """Test /tools and /agent-systems basic workflows."""
    _require_openai()
    from capstone.backend.app.main import app

    async with httpx.AsyncClient(app=app, base_url="http://test") as client:
        # /tools
        tools = await client.get("/tools")
        assert tools.status_code == 200, tools.text
        data = tools.json()
        assert isinstance(data.get("tools"), list) and len(data["tools"]) > 0

        # Register system and fetch it
        sys_id = await _register_minimal_system(client)
        got = await client.get(f"/agent-systems/{sys_id}")
        assert got.status_code == 200, got.text
        # Unknown system
        not_found = await client.get("/agent-systems/does-not-exist")
        assert not_found.status_code == 404


@pytest.mark.asyncio
async def test_health() -> None:
    """Test the /health endpoint is reachable."""
    from capstone.backend.app.main import app

    async with httpx.AsyncClient(app=app, base_url="http://test") as client:
        res = await client.get("/health")
        assert res.status_code == 200
        assert res.json().get("status") == "ok"


@pytest.mark.asyncio
async def test_agent_system_register_invalid_returns_400() -> None:
    """Missing orchestrator should yield 400 from /agent-systems."""
    _require_openai()
    from capstone.backend.app.main import app

    async with httpx.AsyncClient(app=app, base_url="http://test") as client:
        bad_doc: Dict[str, object] = {
            "version": 1,
            "system": {"name": "invalid"},
            "agents": [
                {"id": "worker", "role": "sub-agent", "tools": {"allow": ["validate_project_name_and_type"]}}
            ],
        }
        res = await client.post("/agent-systems", json=bad_doc)
        assert res.status_code == 400


@pytest.mark.asyncio
async def test_session_lifecycle_and_artifacts() -> None:
    """Test session create → message → stream → state → artifact download."""
    _require_openai()
    from capstone.backend.app.main import app

    async with httpx.AsyncClient(app=app, base_url="http://test", timeout=httpx.Timeout(60.0)) as client:
        sys_id = await _register_minimal_system(client)
        ses = await client.post("/sessions", json={"agent_system_id": sys_id})
        assert ses.status_code == 201, ses.text
        sid = ses.json()["sid"]

        sent = await client.post(f"/sessions/{sid}/messages", json={"text": "Validate project demo-service"})
        assert sent.status_code == 202, sent.text

        # Read a handful of SSE events to allow initial plan creation
        async with client.stream("GET", f"/sessions/{sid}/stream") as resp:
            assert resp.status_code == 200
            got = 0
            async for line in resp.aiter_lines():
                if line and line.startswith("data: "):
                    got += 1
                if got >= 3:
                    break
        assert got >= 1

        # State
        st = await client.get(f"/sessions/{sid}/state")
        assert st.status_code == 200
        state = st.json()
        assert "version" in state and isinstance(state.get("tasks"), list)

        # Artifact
        art = await client.get(f"/sessions/{sid}/artifacts/todolist.md")
        assert art.status_code == 200
        assert art.headers.get("content-type", "").startswith("application/octet-stream") or art.text.startswith("# Todo List")


@pytest.mark.asyncio
async def test_unknown_session_404s() -> None:
    """Unknown session should return 404 across endpoints."""
    _require_openai()
    from capstone.backend.app.main import app

    async with httpx.AsyncClient(app=app, base_url="http://test") as client:
        s1 = await client.get("/sessions/does-not-exist/state")
        assert s1.status_code == 404
        s2 = await client.get("/sessions/does-not-exist/stream")
        assert s2.status_code == 404
        # Artifact path for unknown session should 404 as file doesn't exist
        s3 = await client.get("/sessions/does-not-exist/artifacts/todolist.md")
        assert s3.status_code in (404, 200)  # implementation may not check registry; tolerate missing


@pytest.mark.asyncio
async def test_answers_and_404s() -> None:
    """Test /answers endpoint and 404 responses for unknown resources."""
    _require_openai()
    from capstone.backend.app.main import app

    async with httpx.AsyncClient(app=app, base_url="http://test") as client:
        sys_id = await _register_minimal_system(client)
        ses = await client.post("/sessions", json={"agent_system_id": sys_id})
        assert ses.status_code == 201
        sid = ses.json()["sid"]

        # /answers should accept even if not awaiting input (no-op safety)
        ans = await client.post(f"/sessions/{sid}/answers", json={"text": "Some answer"})
        assert ans.status_code == 202

        # Unknown session
        s404 = await client.get("/sessions/does-not-exist/state")
        assert s404.status_code == 404






// Relative Path: tests\test_artifacts.py
from __future__ import annotations

"""Artifact lifecycle test: 404 before processing, 200 after stream produces Markdown."""

import os
import httpx
import pytest


def _require_openai() -> None:
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    assert api_key, "OPENAI_API_KEY must be set to run artifact tests"


@pytest.mark.asyncio
async def test_artifact_created_after_stream() -> None:
    _require_openai()
    from capstone.backend.app.main import app

    async with httpx.AsyncClient(app=app, base_url="http://test", timeout=httpx.Timeout(60.0)) as client:
        # Register minimal system
        reg = await client.post(
            "/agent-systems",
            json={
                "version": 1,
                "system": {"name": "artifact-flow"},
                "agents": [
                    {
                        "id": "orchestrator",
                        "role": "orchestrator",
                        "system_prompt": "You are the orchestrator.",
                        "mission": "Create a plan and render todolist.",
                        "max_steps": 3,
                        "tools": {"allow": ["validate_project_name_and_type"]},
                        "model": {"provider": "openai", "model": "gpt-4.1", "temperature": 0.1},
                    }
                ],
            },
        )
        assert reg.status_code == 201
        sys_id = reg.json()["id"]

        # Create session
        ses = await client.post("/sessions", json={"agent_system_id": sys_id})
        assert ses.status_code == 201
        sid = ses.json()["sid"]

        # Artifact should be missing before any processing
        pre = await client.get(f"/sessions/{sid}/artifacts/todolist.md")
        assert pre.status_code == 404

        # Send message
        sent = await client.post(
            f"/sessions/{sid}/messages",
            json={"text": "Validate project demo-service and start."},
        )
        assert sent.status_code == 202

        # Consume some events to trigger plan creation + rendering
        async with client.stream("GET", f"/sessions/{sid}/stream") as resp:
            assert resp.status_code == 200
            got = 0
            async for line in resp.aiter_lines():
                if line and line.startswith("data: "):
                    got += 1
                if got >= 3:
                    break
        assert got >= 1

        # Artifact should now exist
        post = await client.get(f"/sessions/{sid}/artifacts/todolist.md")
        assert post.status_code == 200
        assert post.text.startswith("# Todo List")






// Relative Path: tests\test_e2e.py
from __future__ import annotations

import asyncio
import os
from typing import List

import httpx
import pytest


@pytest.mark.asyncio
async def test_e2e_register_session_stream_state() -> None:
    """End-to-end test using the real OpenAI provider if OPENAI_API_KEY is set.

    Flow:
    - Register a minimal AgentSystem (orchestrator only)
    - Create a session
    - Send a message
    - Consume a few SSE events from the stream
    - Inspect session state
    """

    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    assert api_key, "OPENAI_API_KEY must be set to run integration tests with real LLM"

    from capstone.backend.app.main import app

    timeout = httpx.Timeout(connect=20.0, read=30.0, write=20.0, pool=20.0)
    async with httpx.AsyncClient(app=app, base_url="http://test", timeout=timeout) as client:
        register_doc = {
            "version": 1,
            "system": {"name": "e2e-smoke"},
            "agents": [
                {
                    "id": "orchestrator",
                    "role": "orchestrator",
                    "system_prompt": "You are the orchestrator.",
                    "mission": "Create a short plan and validate inputs.",
                    "max_steps": 3,
                    "tools": {"allow": ["validate_project_name_and_type"]},
                    "model": {"provider": "openai", "model": "gpt-4.1", "temperature": 0.1},
                }
            ],
        }
        reg = await client.post("/agent-systems", json=register_doc)
        assert reg.status_code == 201, reg.text
        system_id = reg.json()["id"]

        ses = await client.post("/sessions", json={"agent_system_id": system_id})
        assert ses.status_code == 201, ses.text
        sid = ses.json()["sid"]

        msg = await client.post(
            f"/sessions/{sid}/messages",
            json={"text": "Please validate project name 'demo-service' and start."},
        )
        assert msg.status_code == 202, msg.text

        events: List[str] = []
        async with client.stream("GET", f"/sessions/{sid}/stream") as resp:
            assert resp.status_code == 200
            async for line in resp.aiter_lines():
                if not line:
                    continue
                if line.startswith("data: "):
                    payload = line[len("data: ") :].strip()
                    if payload:
                        events.append(payload)
                if len(events) >= 3:
                    break
        assert len(events) >= 1, "Expected at least one SSE data event"

        st = await client.get(f"/sessions/{sid}/state")
        assert st.status_code == 200, st.text
        state = st.json()
        assert "version" in state
        assert "tasks" in state






